2025-06-13 17:33:53 Training with configuration:
2025-06-13 17:33:53 data:
2025-06-13 17:33:53   bbox_margin: 20
2025-06-13 17:33:53   colormode: RGB
2025-06-13 17:33:53   inference:
2025-06-13 17:33:53     normalize_images: True
2025-06-13 17:33:53   train:
2025-06-13 17:33:53     affine:
2025-06-13 17:33:53       p: 0.5
2025-06-13 17:33:53       rotation: 30
2025-06-13 17:33:53       scaling: [0.5, 1.25]
2025-06-13 17:33:53       translation: 0
2025-06-13 17:33:53     crop_sampling:
2025-06-13 17:33:53       width: 448
2025-06-13 17:33:53       height: 448
2025-06-13 17:33:53       max_shift: 0.1
2025-06-13 17:33:53       method: hybrid
2025-06-13 17:33:53     gaussian_noise: 12.75
2025-06-13 17:33:53     motion_blur: True
2025-06-13 17:33:53     normalize_images: True
2025-06-13 17:33:53 device: auto
2025-06-13 17:33:53 metadata:
2025-06-13 17:33:53   project_path: /Users/rishi/Desktop/singlelarvae_imaging-rishi-2025-06-13
2025-06-13 17:33:53   pose_config_path: /Users/rishi/Desktop/singlelarvae_imaging-rishi-2025-06-13/dlc-models-pytorch/iteration-0/singlelarvae_imagingJun13-trainset95shuffle1/train/pytorch_config.yaml
2025-06-13 17:33:53   bodyparts: ['left_eye', 'right_eye', 'middle', 'tail', 'well_center']
2025-06-13 17:33:53   unique_bodyparts: []
2025-06-13 17:33:53   individuals: ['animal']
2025-06-13 17:33:53   with_identity: None
2025-06-13 17:33:53 method: bu
2025-06-13 17:33:53 model:
2025-06-13 17:33:53   backbone:
2025-06-13 17:33:53     type: ResNet
2025-06-13 17:33:53     model_name: resnet50_gn
2025-06-13 17:33:53     output_stride: 16
2025-06-13 17:33:53     freeze_bn_stats: False
2025-06-13 17:33:53     freeze_bn_weights: False
2025-06-13 17:33:53   backbone_output_channels: 2048
2025-06-13 17:33:53   heads:
2025-06-13 17:33:53     bodypart:
2025-06-13 17:33:53       type: HeatmapHead
2025-06-13 17:33:53       weight_init: normal
2025-06-13 17:33:53       predictor:
2025-06-13 17:33:53         type: HeatmapPredictor
2025-06-13 17:33:53         apply_sigmoid: False
2025-06-13 17:33:53         clip_scores: True
2025-06-13 17:33:53         location_refinement: True
2025-06-13 17:33:53         locref_std: 7.2801
2025-06-13 17:33:53       target_generator:
2025-06-13 17:33:53         type: HeatmapGaussianGenerator
2025-06-13 17:33:53         num_heatmaps: 5
2025-06-13 17:33:53         pos_dist_thresh: 17
2025-06-13 17:33:53         heatmap_mode: KEYPOINT
2025-06-13 17:33:53         gradient_masking: False
2025-06-13 17:33:53         generate_locref: True
2025-06-13 17:33:53         locref_std: 7.2801
2025-06-13 17:33:53       criterion:
2025-06-13 17:33:53         heatmap:
2025-06-13 17:33:53           type: WeightedMSECriterion
2025-06-13 17:33:53           weight: 1.0
2025-06-13 17:33:53         locref:
2025-06-13 17:33:53           type: WeightedHuberCriterion
2025-06-13 17:33:53           weight: 0.05
2025-06-13 17:33:53       heatmap_config:
2025-06-13 17:33:53         channels: [2048, 5]
2025-06-13 17:33:53         kernel_size: [3]
2025-06-13 17:33:53         strides: [2]
2025-06-13 17:33:53       locref_config:
2025-06-13 17:33:53         channels: [2048, 10]
2025-06-13 17:33:53         kernel_size: [3]
2025-06-13 17:33:53         strides: [2]
2025-06-13 17:33:53 net_type: resnet_50
2025-06-13 17:33:53 runner:
2025-06-13 17:33:53   type: PoseTrainingRunner
2025-06-13 17:33:53   gpus: None
2025-06-13 17:33:53   key_metric: test.mAP
2025-06-13 17:33:53   key_metric_asc: True
2025-06-13 17:33:53   eval_interval: 10
2025-06-13 17:33:53   optimizer:
2025-06-13 17:33:53     type: AdamW
2025-06-13 17:33:53     params:
2025-06-13 17:33:53       lr: 0.0005
2025-06-13 17:33:53   scheduler:
2025-06-13 17:33:53     type: LRListScheduler
2025-06-13 17:33:53     params:
2025-06-13 17:33:53       lr_list: [[0.0001], [1e-05]]
2025-06-13 17:33:53       milestones: [90, 120]
2025-06-13 17:33:53   snapshots:
2025-06-13 17:33:53     max_snapshots: 5
2025-06-13 17:33:53     save_epochs: 25
2025-06-13 17:33:53     save_optimizer_state: False
2025-06-13 17:33:53 train_settings:
2025-06-13 17:33:53   batch_size: 8
2025-06-13 17:33:53   dataloader_workers: 0
2025-06-13 17:33:53   dataloader_pin_memory: False
2025-06-13 17:33:53   display_iters: 500
2025-06-13 17:33:53   epochs: 200
2025-06-13 17:33:53   seed: 42
2025-06-13 17:33:54 Loading pretrained weights from Hugging Face hub (timm/resnet50_gn.a1h_in1k)
2025-06-13 17:33:54 [timm/resnet50_gn.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-06-13 17:33:55 Data Transforms:
2025-06-13 17:33:55   Training:   Compose([
  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (0.5, 1.25), 'y': (0.5, 1.25)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),
  PadIfNeeded(always_apply=True, p=1.0, min_height=448, min_width=448, pad_height_divisor=None, pad_width_divisor=None, position=PositionType.CENTER, border_mode=0, value=None, mask_value=None),
  KeypointAwareCrop(always_apply=True, p=1.0, width=448, height=448, max_shift=0.1, crop_sampling='hybrid'),
  MotionBlur(always_apply=False, p=0.5, blur_limit=(3, 7), allow_shifted=True),
  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2025-06-13 17:33:55   Validation: Compose([
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2025-06-13 17:33:56 Using 70 images and 4 for testing
2025-06-13 17:33:56 
Starting pose model training...
--------------------------------------------------
2025-06-13 17:34:01 Epoch 1/200 (lr=0.0005), train loss 0.01518
2025-06-13 17:34:03 Epoch 2/200 (lr=0.0005), train loss 0.01257
2025-06-13 17:34:06 Epoch 3/200 (lr=0.0005), train loss 0.01009
2025-06-13 17:34:09 Epoch 4/200 (lr=0.0005), train loss 0.00882
2025-06-13 17:34:11 Epoch 5/200 (lr=0.0005), train loss 0.00772
2025-06-13 17:34:14 Epoch 6/200 (lr=0.0005), train loss 0.00643
2025-06-13 17:34:16 Epoch 7/200 (lr=0.0005), train loss 0.00503
2025-06-13 17:34:19 Epoch 8/200 (lr=0.0005), train loss 0.00495
2025-06-13 17:34:21 Epoch 9/200 (lr=0.0005), train loss 0.00416
2025-06-13 17:34:24 Training for epoch 10 done, starting evaluation
2025-06-13 17:34:24 Epoch 10/200 (lr=0.0005), train loss 0.00337, valid loss 0.00799
2025-06-13 17:34:24 Model performance:
2025-06-13 17:34:24   metrics/test.rmse:          12.82
2025-06-13 17:34:24   metrics/test.rmse_pcutoff:   3.79
2025-06-13 17:34:24   metrics/test.mAP:           52.72
2025-06-13 17:34:24   metrics/test.mAR:           60.00
2025-06-13 17:34:27 Epoch 11/200 (lr=0.0005), train loss 0.00286
2025-06-13 17:34:29 Epoch 12/200 (lr=0.0005), train loss 0.00244
2025-06-13 17:34:32 Epoch 13/200 (lr=0.0005), train loss 0.00203
2025-06-13 17:34:34 Epoch 14/200 (lr=0.0005), train loss 0.00187
2025-06-13 17:34:37 Epoch 15/200 (lr=0.0005), train loss 0.00161
2025-06-13 17:34:39 Epoch 16/200 (lr=0.0005), train loss 0.00152
2025-06-13 17:34:42 Epoch 17/200 (lr=0.0005), train loss 0.00130
2025-06-13 17:34:44 Epoch 18/200 (lr=0.0005), train loss 0.00148
2025-06-13 17:34:47 Epoch 19/200 (lr=0.0005), train loss 0.00166
2025-06-13 17:34:50 Training for epoch 20 done, starting evaluation
2025-06-13 17:34:50 Epoch 20/200 (lr=0.0005), train loss 0.00139, valid loss 0.00635
2025-06-13 17:34:50 Model performance:
2025-06-13 17:34:50   metrics/test.rmse:           4.60
2025-06-13 17:34:50   metrics/test.rmse_pcutoff:   3.21
2025-06-13 17:34:50   metrics/test.mAP:           69.08
2025-06-13 17:34:50   metrics/test.mAR:           80.00
2025-06-13 17:34:53 Epoch 21/200 (lr=0.0005), train loss 0.00125
2025-06-13 17:34:55 Epoch 22/200 (lr=0.0005), train loss 0.00128
2025-06-13 17:34:58 Epoch 23/200 (lr=0.0005), train loss 0.00131
2025-06-13 17:35:00 Epoch 24/200 (lr=0.0005), train loss 0.00102
2025-06-13 17:35:03 Epoch 25/200 (lr=0.0005), train loss 0.00105
2025-06-13 17:35:06 Epoch 26/200 (lr=0.0005), train loss 0.00111
2025-06-13 17:35:08 Epoch 27/200 (lr=0.0005), train loss 0.00112
2025-06-13 17:35:11 Epoch 28/200 (lr=0.0005), train loss 0.00105
2025-06-13 17:35:13 Epoch 29/200 (lr=0.0005), train loss 0.00119
2025-06-13 17:35:16 Training for epoch 30 done, starting evaluation
2025-06-13 17:35:16 Epoch 30/200 (lr=0.0005), train loss 0.00112, valid loss 0.00618
2025-06-13 17:35:16 Model performance:
2025-06-13 17:35:16   metrics/test.rmse:           5.21
2025-06-13 17:35:16   metrics/test.rmse_pcutoff:   3.70
2025-06-13 17:35:16   metrics/test.mAP:           72.08
2025-06-13 17:35:16   metrics/test.mAR:           80.00
2025-06-13 17:35:18 Epoch 31/200 (lr=0.0005), train loss 0.00095
2025-06-13 17:35:21 Epoch 32/200 (lr=0.0005), train loss 0.00083
2025-06-13 17:35:23 Epoch 33/200 (lr=0.0005), train loss 0.00082
2025-06-13 17:35:26 Epoch 34/200 (lr=0.0005), train loss 0.00083
2025-06-13 17:35:29 Epoch 35/200 (lr=0.0005), train loss 0.00084
2025-06-13 17:35:32 Epoch 36/200 (lr=0.0005), train loss 0.00084
2025-06-13 17:35:34 Epoch 37/200 (lr=0.0005), train loss 0.00093
2025-06-13 17:35:37 Epoch 38/200 (lr=0.0005), train loss 0.00098
2025-06-13 17:35:39 Epoch 39/200 (lr=0.0005), train loss 0.00095
2025-06-13 17:35:42 Training for epoch 40 done, starting evaluation
2025-06-13 17:35:42 Epoch 40/200 (lr=0.0005), train loss 0.00090, valid loss 0.00515
2025-06-13 17:35:42 Model performance:
2025-06-13 17:35:42   metrics/test.rmse:           3.88
2025-06-13 17:35:42   metrics/test.rmse_pcutoff:   2.21
2025-06-13 17:35:42   metrics/test.mAP:           87.62
2025-06-13 17:35:42   metrics/test.mAR:           90.00
2025-06-13 17:35:44 Epoch 41/200 (lr=0.0005), train loss 0.00083
2025-06-13 17:35:47 Epoch 42/200 (lr=0.0005), train loss 0.00093
2025-06-13 17:35:50 Epoch 43/200 (lr=0.0005), train loss 0.00086
2025-06-13 17:35:52 Epoch 44/200 (lr=0.0005), train loss 0.00078
2025-06-13 17:35:55 Epoch 45/200 (lr=0.0005), train loss 0.00078
2025-06-13 17:35:57 Epoch 46/200 (lr=0.0005), train loss 0.00085
2025-06-13 17:36:00 Epoch 47/200 (lr=0.0005), train loss 0.00091
2025-06-13 17:36:02 Epoch 48/200 (lr=0.0005), train loss 0.00073
2025-06-13 17:36:05 Epoch 49/200 (lr=0.0005), train loss 0.00063
2025-06-13 17:36:08 Training for epoch 50 done, starting evaluation
2025-06-13 17:36:08 Epoch 50/200 (lr=0.0005), train loss 0.00075, valid loss 0.00493
2025-06-13 17:36:08 Model performance:
2025-06-13 17:36:08   metrics/test.rmse:           4.29
2025-06-13 17:36:08   metrics/test.rmse_pcutoff:   2.69
2025-06-13 17:36:08   metrics/test.mAP:           80.72
2025-06-13 17:36:08   metrics/test.mAR:           85.00
2025-06-13 17:36:10 Epoch 51/200 (lr=0.0005), train loss 0.00080
2025-06-13 17:36:13 Epoch 52/200 (lr=0.0005), train loss 0.00076
2025-06-13 17:36:15 Epoch 53/200 (lr=0.0005), train loss 0.00087
2025-06-13 17:36:18 Epoch 54/200 (lr=0.0005), train loss 0.00078
2025-06-13 17:36:20 Epoch 55/200 (lr=0.0005), train loss 0.00068
2025-06-13 17:36:23 Epoch 56/200 (lr=0.0005), train loss 0.00066
2025-06-13 17:36:25 Epoch 57/200 (lr=0.0005), train loss 0.00060
2025-06-13 17:36:28 Epoch 58/200 (lr=0.0005), train loss 0.00060
2025-06-13 17:36:31 Epoch 59/200 (lr=0.0005), train loss 0.00054
2025-06-13 17:36:33 Training for epoch 60 done, starting evaluation
2025-06-13 17:36:33 Epoch 60/200 (lr=0.0005), train loss 0.00056, valid loss 0.00454
2025-06-13 17:36:33 Model performance:
2025-06-13 17:36:33   metrics/test.rmse:           8.32
2025-06-13 17:36:33   metrics/test.rmse_pcutoff:   2.37
2025-06-13 17:36:33   metrics/test.mAP:           73.53
2025-06-13 17:36:33   metrics/test.mAR:           77.50
2025-06-13 17:36:36 Epoch 61/200 (lr=0.0005), train loss 0.00057
2025-06-13 17:36:38 Epoch 62/200 (lr=0.0005), train loss 0.00059
2025-06-13 17:36:41 Epoch 63/200 (lr=0.0005), train loss 0.00069
2025-06-13 17:36:43 Epoch 64/200 (lr=0.0005), train loss 0.00073
2025-06-13 17:36:46 Epoch 65/200 (lr=0.0005), train loss 0.00060
2025-06-13 17:36:48 Epoch 66/200 (lr=0.0005), train loss 0.00061
2025-06-13 17:36:51 Epoch 67/200 (lr=0.0005), train loss 0.00056
2025-06-13 17:36:54 Epoch 68/200 (lr=0.0005), train loss 0.00065
2025-06-13 17:36:56 Epoch 69/200 (lr=0.0005), train loss 0.00067
2025-06-13 17:36:59 Training for epoch 70 done, starting evaluation
2025-06-13 17:36:59 Epoch 70/200 (lr=0.0005), train loss 0.00079, valid loss 0.00499
2025-06-13 17:36:59 Model performance:
2025-06-13 17:36:59   metrics/test.rmse:           4.46
2025-06-13 17:36:59   metrics/test.rmse_pcutoff:   3.20
2025-06-13 17:36:59   metrics/test.mAP:           79.72
2025-06-13 17:36:59   metrics/test.mAR:           85.00
2025-06-13 17:37:01 Epoch 71/200 (lr=0.0005), train loss 0.00069
2025-06-13 17:37:04 Epoch 72/200 (lr=0.0005), train loss 0.00063
2025-06-13 17:37:06 Epoch 73/200 (lr=0.0005), train loss 0.00061
2025-06-13 17:37:09 Epoch 74/200 (lr=0.0005), train loss 0.00059
2025-06-13 17:37:12 Epoch 75/200 (lr=0.0005), train loss 0.00060
2025-06-13 17:37:14 Epoch 76/200 (lr=0.0005), train loss 0.00063
2025-06-13 17:37:17 Epoch 77/200 (lr=0.0005), train loss 0.00060
2025-06-13 17:37:19 Epoch 78/200 (lr=0.0005), train loss 0.00049
2025-06-13 17:37:22 Epoch 79/200 (lr=0.0005), train loss 0.00062
2025-06-13 17:37:24 Training for epoch 80 done, starting evaluation
2025-06-13 17:37:24 Epoch 80/200 (lr=0.0005), train loss 0.00054, valid loss 0.00444
2025-06-13 17:37:24 Model performance:
2025-06-13 17:37:24   metrics/test.rmse:           4.03
2025-06-13 17:37:24   metrics/test.rmse_pcutoff:   3.27
2025-06-13 17:37:24   metrics/test.mAP:           81.93
2025-06-13 17:37:24   metrics/test.mAR:           87.50
2025-06-13 17:37:27 Epoch 81/200 (lr=0.0005), train loss 0.00059
2025-06-13 17:37:29 Epoch 82/200 (lr=0.0005), train loss 0.00046
2025-06-13 17:37:32 Epoch 83/200 (lr=0.0005), train loss 0.00047
2025-06-13 17:37:34 Epoch 84/200 (lr=0.0005), train loss 0.00049
2025-06-13 17:37:37 Epoch 85/200 (lr=0.0005), train loss 0.00048
2025-06-13 17:37:39 Epoch 86/200 (lr=0.0005), train loss 0.00048
2025-06-13 17:37:42 Epoch 87/200 (lr=0.0005), train loss 0.00055
2025-06-13 17:37:44 Epoch 88/200 (lr=0.0005), train loss 0.00052
2025-06-13 17:37:47 Epoch 89/200 (lr=0.0005), train loss 0.00051
2025-06-13 17:37:49 Training for epoch 90 done, starting evaluation
2025-06-13 17:37:49 Epoch 90/200 (lr=0.0001), train loss 0.00058, valid loss 0.00408
2025-06-13 17:37:49 Model performance:
2025-06-13 17:37:49   metrics/test.rmse:           3.62
2025-06-13 17:37:49   metrics/test.rmse_pcutoff:   1.86
2025-06-13 17:37:49   metrics/test.mAP:           81.96
2025-06-13 17:37:49   metrics/test.mAR:           87.50
2025-06-13 17:37:52 Epoch 91/200 (lr=0.0001), train loss 0.00050
2025-06-13 17:37:54 Epoch 92/200 (lr=0.0001), train loss 0.00037
2025-06-13 17:37:57 Epoch 93/200 (lr=0.0001), train loss 0.00037
2025-06-13 17:38:00 Epoch 94/200 (lr=0.0001), train loss 0.00037
2025-06-13 17:38:02 Epoch 95/200 (lr=0.0001), train loss 0.00029
2025-06-13 17:38:05 Epoch 96/200 (lr=0.0001), train loss 0.00029
2025-06-13 17:38:07 Epoch 97/200 (lr=0.0001), train loss 0.00035
2025-06-13 17:38:10 Epoch 98/200 (lr=0.0001), train loss 0.00029
2025-06-13 17:38:12 Epoch 99/200 (lr=0.0001), train loss 0.00030
2025-06-13 17:38:15 Training for epoch 100 done, starting evaluation
2025-06-13 17:38:15 Epoch 100/200 (lr=0.0001), train loss 0.00020, valid loss 0.00392
2025-06-13 17:38:15 Model performance:
2025-06-13 17:38:15   metrics/test.rmse:           3.56
2025-06-13 17:38:15   metrics/test.rmse_pcutoff:   2.60
2025-06-13 17:38:15   metrics/test.mAP:           81.34
2025-06-13 17:38:15   metrics/test.mAR:           87.50
2025-06-13 17:38:18 Epoch 101/200 (lr=0.0001), train loss 0.00026
2025-06-13 17:38:20 Epoch 102/200 (lr=0.0001), train loss 0.00030
2025-06-13 17:38:23 Epoch 103/200 (lr=0.0001), train loss 0.00029
2025-06-13 17:38:25 Epoch 104/200 (lr=0.0001), train loss 0.00028
2025-06-13 17:38:28 Epoch 105/200 (lr=0.0001), train loss 0.00033
2025-06-13 17:38:31 Epoch 106/200 (lr=0.0001), train loss 0.00030
2025-06-13 17:38:33 Epoch 107/200 (lr=0.0001), train loss 0.00031
2025-06-13 17:38:36 Epoch 108/200 (lr=0.0001), train loss 0.00030
2025-06-13 17:38:38 Epoch 109/200 (lr=0.0001), train loss 0.00029
2025-06-13 17:38:41 Training for epoch 110 done, starting evaluation
2025-06-13 17:38:41 Epoch 110/200 (lr=0.0001), train loss 0.00027, valid loss 0.00394
2025-06-13 17:38:41 Model performance:
2025-06-13 17:38:41   metrics/test.rmse:           3.65
2025-06-13 17:38:41   metrics/test.rmse_pcutoff:   2.62
2025-06-13 17:38:41   metrics/test.mAP:           85.10
2025-06-13 17:38:41   metrics/test.mAR:           90.00
2025-06-13 17:38:43 Epoch 111/200 (lr=0.0001), train loss 0.00031
2025-06-13 17:38:46 Epoch 112/200 (lr=0.0001), train loss 0.00027
2025-06-13 17:38:48 Epoch 113/200 (lr=0.0001), train loss 0.00031
2025-06-13 17:38:51 Epoch 114/200 (lr=0.0001), train loss 0.00027
2025-06-13 17:38:53 Epoch 115/200 (lr=0.0001), train loss 0.00025
2025-06-13 17:38:56 Epoch 116/200 (lr=0.0001), train loss 0.00022
2025-06-13 17:38:59 Epoch 117/200 (lr=0.0001), train loss 0.00025
2025-06-13 17:39:01 Epoch 118/200 (lr=0.0001), train loss 0.00032
2025-06-13 17:39:03 Epoch 119/200 (lr=0.0001), train loss 0.00029
2025-06-13 17:39:06 Training for epoch 120 done, starting evaluation
2025-06-13 17:39:06 Epoch 120/200 (lr=1e-05), train loss 0.00018, valid loss 0.00390
2025-06-13 17:39:06 Model performance:
2025-06-13 17:39:06   metrics/test.rmse:           3.57
2025-06-13 17:39:06   metrics/test.rmse_pcutoff:   2.49
2025-06-13 17:39:06   metrics/test.mAP:           81.34
2025-06-13 17:39:06   metrics/test.mAR:           87.50
2025-06-13 17:39:09 Epoch 121/200 (lr=1e-05), train loss 0.00026
2025-06-13 17:39:11 Epoch 122/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:39:14 Epoch 123/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:39:16 Epoch 124/200 (lr=1e-05), train loss 0.00025
2025-06-13 17:39:19 Epoch 125/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:39:22 Epoch 126/200 (lr=1e-05), train loss 0.00030
2025-06-13 17:39:24 Epoch 127/200 (lr=1e-05), train loss 0.00025
2025-06-13 17:39:27 Epoch 128/200 (lr=1e-05), train loss 0.00026
2025-06-13 17:39:29 Epoch 129/200 (lr=1e-05), train loss 0.00027
2025-06-13 17:39:32 Training for epoch 130 done, starting evaluation
2025-06-13 17:39:32 Epoch 130/200 (lr=1e-05), train loss 0.00024, valid loss 0.00396
2025-06-13 17:39:32 Model performance:
2025-06-13 17:39:32   metrics/test.rmse:           3.87
2025-06-13 17:39:32   metrics/test.rmse_pcutoff:   2.49
2025-06-13 17:39:32   metrics/test.mAP:           79.67
2025-06-13 17:39:32   metrics/test.mAR:           85.00
2025-06-13 17:39:34 Epoch 131/200 (lr=1e-05), train loss 0.00032
2025-06-13 17:39:37 Epoch 132/200 (lr=1e-05), train loss 0.00024
2025-06-13 17:39:39 Epoch 133/200 (lr=1e-05), train loss 0.00026
2025-06-13 17:39:42 Epoch 134/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:39:44 Epoch 135/200 (lr=1e-05), train loss 0.00029
2025-06-13 17:39:47 Epoch 136/200 (lr=1e-05), train loss 0.00020
2025-06-13 17:39:49 Epoch 137/200 (lr=1e-05), train loss 0.00025
2025-06-13 17:39:52 Epoch 138/200 (lr=1e-05), train loss 0.00024
2025-06-13 17:39:54 Epoch 139/200 (lr=1e-05), train loss 0.00027
2025-06-13 17:39:57 Training for epoch 140 done, starting evaluation
2025-06-13 17:39:57 Epoch 140/200 (lr=1e-05), train loss 0.00028, valid loss 0.00394
2025-06-13 17:39:57 Model performance:
2025-06-13 17:39:57   metrics/test.rmse:           3.83
2025-06-13 17:39:57   metrics/test.rmse_pcutoff:   2.45
2025-06-13 17:39:57   metrics/test.mAP:           79.67
2025-06-13 17:39:57   metrics/test.mAR:           85.00
2025-06-13 17:40:00 Epoch 141/200 (lr=1e-05), train loss 0.00020
2025-06-13 17:40:02 Epoch 142/200 (lr=1e-05), train loss 0.00025
2025-06-13 17:40:05 Epoch 143/200 (lr=1e-05), train loss 0.00030
2025-06-13 17:40:07 Epoch 144/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:40:10 Epoch 145/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:40:12 Epoch 146/200 (lr=1e-05), train loss 0.00020
2025-06-13 17:40:15 Epoch 147/200 (lr=1e-05), train loss 0.00029
2025-06-13 17:40:17 Epoch 148/200 (lr=1e-05), train loss 0.00021
2025-06-13 17:40:20 Epoch 149/200 (lr=1e-05), train loss 0.00026
2025-06-13 17:40:23 Training for epoch 150 done, starting evaluation
2025-06-13 17:40:23 Epoch 150/200 (lr=1e-05), train loss 0.00020, valid loss 0.00393
2025-06-13 17:40:23 Model performance:
2025-06-13 17:40:23   metrics/test.rmse:           3.65
2025-06-13 17:40:23   metrics/test.rmse_pcutoff:   2.42
2025-06-13 17:40:23   metrics/test.mAP:           83.43
2025-06-13 17:40:23   metrics/test.mAR:           87.50
2025-06-13 17:40:25 Epoch 151/200 (lr=1e-05), train loss 0.00025
2025-06-13 17:40:28 Epoch 152/200 (lr=1e-05), train loss 0.00020
2025-06-13 17:40:31 Epoch 153/200 (lr=1e-05), train loss 0.00021
2025-06-13 17:40:33 Epoch 154/200 (lr=1e-05), train loss 0.00026
2025-06-13 17:40:36 Epoch 155/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:40:38 Epoch 156/200 (lr=1e-05), train loss 0.00033
2025-06-13 17:40:41 Epoch 157/200 (lr=1e-05), train loss 0.00022
2025-06-13 17:40:43 Epoch 158/200 (lr=1e-05), train loss 0.00021
2025-06-13 17:40:46 Epoch 159/200 (lr=1e-05), train loss 0.00030
2025-06-13 17:40:48 Training for epoch 160 done, starting evaluation
2025-06-13 17:40:48 Epoch 160/200 (lr=1e-05), train loss 0.00026, valid loss 0.00394
2025-06-13 17:40:48 Model performance:
2025-06-13 17:40:48   metrics/test.rmse:           3.66
2025-06-13 17:40:48   metrics/test.rmse_pcutoff:   2.43
2025-06-13 17:40:48   metrics/test.mAP:           83.43
2025-06-13 17:40:48   metrics/test.mAR:           87.50
2025-06-13 17:40:51 Epoch 161/200 (lr=1e-05), train loss 0.00024
2025-06-13 17:40:53 Epoch 162/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:40:56 Epoch 163/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:40:58 Epoch 164/200 (lr=1e-05), train loss 0.00026
2025-06-13 17:41:01 Epoch 165/200 (lr=1e-05), train loss 0.00026
2025-06-13 17:41:04 Epoch 166/200 (lr=1e-05), train loss 0.00027
2025-06-13 17:41:06 Epoch 167/200 (lr=1e-05), train loss 0.00019
2025-06-13 17:41:09 Epoch 168/200 (lr=1e-05), train loss 0.00029
2025-06-13 17:41:11 Epoch 169/200 (lr=1e-05), train loss 0.00025
2025-06-13 17:41:14 Training for epoch 170 done, starting evaluation
2025-06-13 17:41:14 Epoch 170/200 (lr=1e-05), train loss 0.00031, valid loss 0.00395
2025-06-13 17:41:14 Model performance:
2025-06-13 17:41:14   metrics/test.rmse:           3.67
2025-06-13 17:41:14   metrics/test.rmse_pcutoff:   2.42
2025-06-13 17:41:14   metrics/test.mAP:           83.43
2025-06-13 17:41:14   metrics/test.mAR:           87.50
2025-06-13 17:41:17 Epoch 171/200 (lr=1e-05), train loss 0.00020
2025-06-13 17:41:19 Epoch 172/200 (lr=1e-05), train loss 0.00020
2025-06-13 17:41:22 Epoch 173/200 (lr=1e-05), train loss 0.00025
2025-06-13 17:41:24 Epoch 174/200 (lr=1e-05), train loss 0.00022
2025-06-13 17:41:27 Epoch 175/200 (lr=1e-05), train loss 0.00029
2025-06-13 17:41:29 Epoch 176/200 (lr=1e-05), train loss 0.00024
2025-06-13 17:41:32 Epoch 177/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:41:35 Epoch 178/200 (lr=1e-05), train loss 0.00025
2025-06-13 17:41:37 Epoch 179/200 (lr=1e-05), train loss 0.00020
2025-06-13 17:41:40 Training for epoch 180 done, starting evaluation
2025-06-13 17:41:40 Epoch 180/200 (lr=1e-05), train loss 0.00026, valid loss 0.00394
2025-06-13 17:41:40 Model performance:
2025-06-13 17:41:40   metrics/test.rmse:           3.68
2025-06-13 17:41:40   metrics/test.rmse_pcutoff:   2.44
2025-06-13 17:41:40   metrics/test.mAP:           83.43
2025-06-13 17:41:40   metrics/test.mAR:           87.50
2025-06-13 17:41:42 Epoch 181/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:41:45 Epoch 182/200 (lr=1e-05), train loss 0.00024
2025-06-13 17:41:47 Epoch 183/200 (lr=1e-05), train loss 0.00022
2025-06-13 17:41:50 Epoch 184/200 (lr=1e-05), train loss 0.00019
2025-06-13 17:41:53 Epoch 185/200 (lr=1e-05), train loss 0.00021
2025-06-13 17:41:55 Epoch 186/200 (lr=1e-05), train loss 0.00018
2025-06-13 17:41:58 Epoch 187/200 (lr=1e-05), train loss 0.00028
2025-06-13 17:42:00 Epoch 188/200 (lr=1e-05), train loss 0.00022
2025-06-13 17:42:03 Epoch 189/200 (lr=1e-05), train loss 0.00023
2025-06-13 17:42:05 Training for epoch 190 done, starting evaluation
2025-06-13 17:42:05 Epoch 190/200 (lr=1e-05), train loss 0.00025, valid loss 0.00396
2025-06-13 17:42:05 Model performance:
2025-06-13 17:42:05   metrics/test.rmse:           3.70
2025-06-13 17:42:05   metrics/test.rmse_pcutoff:   2.46
2025-06-13 17:42:05   metrics/test.mAP:           83.43
2025-06-13 17:42:05   metrics/test.mAR:           87.50
2025-06-13 17:42:08 Epoch 191/200 (lr=1e-05), train loss 0.00021
2025-06-13 17:42:10 Epoch 192/200 (lr=1e-05), train loss 0.00026
2025-06-13 17:42:13 Epoch 193/200 (lr=1e-05), train loss 0.00025
2025-06-13 17:42:15 Epoch 194/200 (lr=1e-05), train loss 0.00022
2025-06-13 17:42:18 Epoch 195/200 (lr=1e-05), train loss 0.00022
2025-06-13 17:42:20 Epoch 196/200 (lr=1e-05), train loss 0.00024
2025-06-13 17:42:23 Epoch 197/200 (lr=1e-05), train loss 0.00020
2025-06-13 17:42:25 Epoch 198/200 (lr=1e-05), train loss 0.00027
2025-06-13 17:42:28 Epoch 199/200 (lr=1e-05), train loss 0.00026
2025-06-13 17:42:30 Training for epoch 200 done, starting evaluation
2025-06-13 17:42:31 Epoch 200/200 (lr=1e-05), train loss 0.00024, valid loss 0.00397
2025-06-13 17:42:31 Model performance:
2025-06-13 17:42:31   metrics/test.rmse:           3.68
2025-06-13 17:42:31   metrics/test.rmse_pcutoff:   2.56
2025-06-13 17:42:31   metrics/test.mAP:           83.43
2025-06-13 17:42:31   metrics/test.mAR:           87.50
2025-06-23 11:04:49 Training with configuration:
2025-06-23 11:04:49 data:
2025-06-23 11:04:49   bbox_margin: 20
2025-06-23 11:04:49   colormode: RGB
2025-06-23 11:04:49   inference:
2025-06-23 11:04:49     normalize_images: True
2025-06-23 11:04:49   train:
2025-06-23 11:04:49     affine:
2025-06-23 11:04:49       p: 0.5
2025-06-23 11:04:49       rotation: 30
2025-06-23 11:04:49       scaling: [0.5, 1.25]
2025-06-23 11:04:49       translation: 0
2025-06-23 11:04:49     crop_sampling:
2025-06-23 11:04:49       width: 448
2025-06-23 11:04:49       height: 448
2025-06-23 11:04:49       max_shift: 0.1
2025-06-23 11:04:49       method: hybrid
2025-06-23 11:04:49     gaussian_noise: 12.75
2025-06-23 11:04:49     motion_blur: True
2025-06-23 11:04:49     normalize_images: True
2025-06-23 11:04:49 device: auto
2025-06-23 11:04:49 metadata:
2025-06-23 11:04:49   project_path: /Users/rishi/Library/Application Support/Mountain Duck/Volumes.noindex/transfer.ccv.brown.edu – SFTP.localized/zebrafish_analysis/Manual-Selection/singlelarvae_imaging-rishi-2025-06-13
2025-06-23 11:04:49   pose_config_path: /Users/rishi/Library/Application Support/Mountain Duck/Volumes.noindex/transfer.ccv.brown.edu – SFTP.localized/zebrafish_analysis/Manual-Selection/singlelarvae_imaging-rishi-2025-06-13/dlc-models-pytorch/iteration-0/singlelarvae_imagingJun13-trainset95shuffle1/train/pytorch_config.yaml
2025-06-23 11:04:49   bodyparts: ['left_eye', 'right_eye', 'middle', 'tail', 'well_center']
2025-06-23 11:04:49   unique_bodyparts: []
2025-06-23 11:04:49   individuals: ['animal']
2025-06-23 11:04:49   with_identity: None
2025-06-23 11:04:49 method: bu
2025-06-23 11:04:49 model:
2025-06-23 11:04:49   backbone:
2025-06-23 11:04:49     type: ResNet
2025-06-23 11:04:49     model_name: resnet50_gn
2025-06-23 11:04:49     output_stride: 16
2025-06-23 11:04:49     freeze_bn_stats: False
2025-06-23 11:04:49     freeze_bn_weights: False
2025-06-23 11:04:49   backbone_output_channels: 2048
2025-06-23 11:04:49   heads:
2025-06-23 11:04:49     bodypart:
2025-06-23 11:04:49       type: HeatmapHead
2025-06-23 11:04:49       weight_init: normal
2025-06-23 11:04:49       predictor:
2025-06-23 11:04:49         type: HeatmapPredictor
2025-06-23 11:04:49         apply_sigmoid: False
2025-06-23 11:04:49         clip_scores: True
2025-06-23 11:04:49         location_refinement: True
2025-06-23 11:04:49         locref_std: 7.2801
2025-06-23 11:04:49       target_generator:
2025-06-23 11:04:49         type: HeatmapGaussianGenerator
2025-06-23 11:04:49         num_heatmaps: 5
2025-06-23 11:04:49         pos_dist_thresh: 17
2025-06-23 11:04:49         heatmap_mode: KEYPOINT
2025-06-23 11:04:49         gradient_masking: False
2025-06-23 11:04:49         generate_locref: True
2025-06-23 11:04:49         locref_std: 7.2801
2025-06-23 11:04:49       criterion:
2025-06-23 11:04:49         heatmap:
2025-06-23 11:04:49           type: WeightedMSECriterion
2025-06-23 11:04:49           weight: 1.0
2025-06-23 11:04:49         locref:
2025-06-23 11:04:49           type: WeightedHuberCriterion
2025-06-23 11:04:49           weight: 0.05
2025-06-23 11:04:49       heatmap_config:
2025-06-23 11:04:49         channels: [2048, 5]
2025-06-23 11:04:49         kernel_size: [3]
2025-06-23 11:04:49         strides: [2]
2025-06-23 11:04:49       locref_config:
2025-06-23 11:04:49         channels: [2048, 10]
2025-06-23 11:04:49         kernel_size: [3]
2025-06-23 11:04:49         strides: [2]
2025-06-23 11:04:49 net_type: resnet_50
2025-06-23 11:04:49 runner:
2025-06-23 11:04:49   type: PoseTrainingRunner
2025-06-23 11:04:49   gpus: None
2025-06-23 11:04:49   key_metric: test.mAP
2025-06-23 11:04:49   key_metric_asc: True
2025-06-23 11:04:49   eval_interval: 10
2025-06-23 11:04:49   optimizer:
2025-06-23 11:04:49     type: AdamW
2025-06-23 11:04:49     params:
2025-06-23 11:04:49       lr: 0.0005
2025-06-23 11:04:49   scheduler:
2025-06-23 11:04:49     type: LRListScheduler
2025-06-23 11:04:49     params:
2025-06-23 11:04:49       lr_list: [[0.0001], [1e-05]]
2025-06-23 11:04:49       milestones: [90, 120]
2025-06-23 11:04:49   snapshots:
2025-06-23 11:04:49     max_snapshots: 5
2025-06-23 11:04:49     save_epochs: 25
2025-06-23 11:04:49     save_optimizer_state: False
2025-06-23 11:04:49 train_settings:
2025-06-23 11:04:49   batch_size: 8
2025-06-23 11:04:49   dataloader_workers: 0
2025-06-23 11:04:49   dataloader_pin_memory: False
2025-06-23 11:04:49   display_iters: 500
2025-06-23 11:04:49   epochs: 200
2025-06-23 11:04:49   seed: 42
2025-06-23 11:04:50 Loading pretrained weights from Hugging Face hub (timm/resnet50_gn.a1h_in1k)
2025-06-23 11:04:50 [timm/resnet50_gn.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-06-23 11:04:51 Data Transforms:
2025-06-23 11:04:51   Training:   Compose([
  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (0.5, 1.25), 'y': (0.5, 1.25)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),
  PadIfNeeded(always_apply=True, p=1.0, min_height=448, min_width=448, pad_height_divisor=None, pad_width_divisor=None, position=PositionType.CENTER, border_mode=0, value=None, mask_value=None),
  KeypointAwareCrop(always_apply=True, p=1.0, width=448, height=448, max_shift=0.1, crop_sampling='hybrid'),
  MotionBlur(always_apply=False, p=0.5, blur_limit=(3, 7), allow_shifted=True),
  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2025-06-23 11:04:51   Validation: Compose([
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2025-06-23 11:04:51 Using 89 images and 5 for testing
2025-06-23 11:04:51 
Starting pose model training...
--------------------------------------------------
2025-06-23 11:04:57 Epoch 1/200 (lr=0.0005), train loss 0.01491
2025-06-23 11:05:01 Epoch 2/200 (lr=0.0005), train loss 0.01323
2025-06-23 11:05:05 Epoch 3/200 (lr=0.0005), train loss 0.01100
2025-06-23 11:05:08 Epoch 4/200 (lr=0.0005), train loss 0.00876
2025-06-23 11:05:12 Epoch 5/200 (lr=0.0005), train loss 0.00869
2025-06-23 11:05:16 Epoch 6/200 (lr=0.0005), train loss 0.00754
2025-06-23 11:05:19 Epoch 7/200 (lr=0.0005), train loss 0.00645
2025-06-23 11:05:23 Epoch 8/200 (lr=0.0005), train loss 0.00548
2025-06-23 11:05:27 Epoch 9/200 (lr=0.0005), train loss 0.00511
2025-06-23 11:05:31 Training for epoch 10 done, starting evaluation
2025-06-23 11:05:31 Epoch 10/200 (lr=0.0005), train loss 0.00436, valid loss 0.00838
2025-06-23 11:05:31 Model performance:
2025-06-23 11:05:31   metrics/test.rmse:          26.59
2025-06-23 11:05:31   metrics/test.rmse_pcutoff:   5.92
2025-06-23 11:05:31   metrics/test.mAP:           54.36
2025-06-23 11:05:31   metrics/test.mAR:           56.00
2025-06-23 11:05:35 Epoch 11/200 (lr=0.0005), train loss 0.00416
2025-06-23 11:05:39 Epoch 12/200 (lr=0.0005), train loss 0.00427
2025-06-23 11:05:42 Epoch 13/200 (lr=0.0005), train loss 0.00339
2025-06-23 11:05:46 Epoch 14/200 (lr=0.0005), train loss 0.00300
2025-06-23 11:05:50 Epoch 15/200 (lr=0.0005), train loss 0.00257
2025-06-23 11:05:53 Epoch 16/200 (lr=0.0005), train loss 0.00245
2025-06-23 11:05:57 Epoch 17/200 (lr=0.0005), train loss 0.00343
2025-06-23 11:06:01 Epoch 18/200 (lr=0.0005), train loss 0.00267
2025-06-23 11:06:05 Epoch 19/200 (lr=0.0005), train loss 0.00274
2025-06-23 11:06:08 Training for epoch 20 done, starting evaluation
2025-06-23 11:06:09 Epoch 20/200 (lr=0.0005), train loss 0.00253, valid loss 0.00614
2025-06-23 11:06:09 Model performance:
2025-06-23 11:06:09   metrics/test.rmse:          25.59
2025-06-23 11:06:09   metrics/test.rmse_pcutoff:   2.07
2025-06-23 11:06:09   metrics/test.mAP:           70.56
2025-06-23 11:06:09   metrics/test.mAR:           76.00
2025-06-23 11:06:13 Epoch 21/200 (lr=0.0005), train loss 0.00237
2025-06-23 11:06:17 Epoch 22/200 (lr=0.0005), train loss 0.00235
2025-06-23 11:06:20 Epoch 23/200 (lr=0.0005), train loss 0.00190
2025-06-23 11:06:24 Epoch 24/200 (lr=0.0005), train loss 0.00192
2025-06-23 11:06:28 Epoch 25/200 (lr=0.0005), train loss 0.00177
2025-06-23 11:06:32 Epoch 26/200 (lr=0.0005), train loss 0.00190
2025-06-23 11:06:35 Epoch 27/200 (lr=0.0005), train loss 0.00271
2025-06-23 11:06:39 Epoch 28/200 (lr=0.0005), train loss 0.00213
2025-06-23 11:06:43 Epoch 29/200 (lr=0.0005), train loss 0.00204
2025-06-23 11:06:47 Training for epoch 30 done, starting evaluation
2025-06-23 11:06:47 Epoch 30/200 (lr=0.0005), train loss 0.00178, valid loss 0.00541
2025-06-23 11:06:47 Model performance:
2025-06-23 11:06:47   metrics/test.rmse:          14.76
2025-06-23 11:06:47   metrics/test.rmse_pcutoff:   3.47
2025-06-23 11:06:47   metrics/test.mAP:           72.07
2025-06-23 11:06:47   metrics/test.mAR:           78.00
2025-06-23 11:06:51 Epoch 31/200 (lr=0.0005), train loss 0.00196
2025-06-23 11:06:55 Epoch 32/200 (lr=0.0005), train loss 0.00187
2025-06-23 11:06:58 Epoch 33/200 (lr=0.0005), train loss 0.00168
2025-06-23 11:07:02 Epoch 34/200 (lr=0.0005), train loss 0.00213
2025-06-23 11:07:06 Epoch 35/200 (lr=0.0005), train loss 0.00175
2025-06-23 11:07:10 Epoch 36/200 (lr=0.0005), train loss 0.00185
2025-06-23 11:07:14 Epoch 37/200 (lr=0.0005), train loss 0.00171
2025-06-23 11:07:17 Epoch 38/200 (lr=0.0005), train loss 0.00171
2025-06-23 11:07:21 Epoch 39/200 (lr=0.0005), train loss 0.00168
2025-06-23 11:07:25 Training for epoch 40 done, starting evaluation
2025-06-23 11:07:25 Epoch 40/200 (lr=0.0005), train loss 0.00154, valid loss 0.00499
2025-06-23 11:07:25 Model performance:
2025-06-23 11:07:25   metrics/test.rmse:          15.46
2025-06-23 11:07:25   metrics/test.rmse_pcutoff:   2.63
2025-06-23 11:07:25   metrics/test.mAP:           69.63
2025-06-23 11:07:25   metrics/test.mAR:           76.00
2025-06-23 11:07:29 Epoch 41/200 (lr=0.0005), train loss 0.00177
2025-06-23 11:07:33 Epoch 42/200 (lr=0.0005), train loss 0.00187
2025-06-23 11:07:36 Epoch 43/200 (lr=0.0005), train loss 0.00169
2025-06-23 11:07:40 Epoch 44/200 (lr=0.0005), train loss 0.00177
2025-06-23 11:07:44 Epoch 45/200 (lr=0.0005), train loss 0.00161
2025-06-23 11:07:48 Epoch 46/200 (lr=0.0005), train loss 0.00146
2025-06-23 11:07:51 Epoch 47/200 (lr=0.0005), train loss 0.00146
2025-06-23 11:07:55 Epoch 48/200 (lr=0.0005), train loss 0.00154
2025-06-23 11:07:59 Epoch 49/200 (lr=0.0005), train loss 0.00151
2025-06-23 11:08:02 Training for epoch 50 done, starting evaluation
2025-06-23 11:08:03 Epoch 50/200 (lr=0.0005), train loss 0.00176, valid loss 0.00433
2025-06-23 11:08:03 Model performance:
2025-06-23 11:08:03   metrics/test.rmse:          14.00
2025-06-23 11:08:03   metrics/test.rmse_pcutoff:   2.69
2025-06-23 11:08:03   metrics/test.mAP:           72.70
2025-06-23 11:08:03   metrics/test.mAR:           82.00
2025-06-23 11:08:06 Epoch 51/200 (lr=0.0005), train loss 0.00138
2025-06-23 11:08:10 Epoch 52/200 (lr=0.0005), train loss 0.00140
2025-06-23 11:08:14 Epoch 53/200 (lr=0.0005), train loss 0.00138
2025-06-23 11:08:18 Epoch 54/200 (lr=0.0005), train loss 0.00138
2025-06-23 11:08:21 Epoch 55/200 (lr=0.0005), train loss 0.00140
2025-06-23 11:08:25 Epoch 56/200 (lr=0.0005), train loss 0.00139
2025-06-23 11:08:29 Epoch 57/200 (lr=0.0005), train loss 0.00131
2025-06-23 11:08:33 Epoch 58/200 (lr=0.0005), train loss 0.00124
2025-06-23 11:08:37 Epoch 59/200 (lr=0.0005), train loss 0.00128
2025-06-23 11:08:40 Training for epoch 60 done, starting evaluation
2025-06-23 11:08:41 Epoch 60/200 (lr=0.0005), train loss 0.00137, valid loss 0.00421
2025-06-23 11:08:41 Model performance:
2025-06-23 11:08:41   metrics/test.rmse:           7.38
2025-06-23 11:08:41   metrics/test.rmse_pcutoff:   2.63
2025-06-23 11:08:41   metrics/test.mAP:           79.41
2025-06-23 11:08:41   metrics/test.mAR:           80.00
2025-06-23 11:08:44 Epoch 61/200 (lr=0.0005), train loss 0.00115
2025-06-23 11:08:48 Epoch 62/200 (lr=0.0005), train loss 0.00116
2025-06-23 11:08:52 Epoch 63/200 (lr=0.0005), train loss 0.00148
2025-06-23 11:08:55 Epoch 64/200 (lr=0.0005), train loss 0.00140
2025-06-23 11:08:59 Epoch 65/200 (lr=0.0005), train loss 0.00139
2025-06-23 11:09:03 Epoch 66/200 (lr=0.0005), train loss 0.00143
2025-06-23 11:09:06 Epoch 67/200 (lr=0.0005), train loss 0.00151
2025-06-23 11:09:10 Epoch 68/200 (lr=0.0005), train loss 0.00176
2025-06-23 11:09:14 Epoch 69/200 (lr=0.0005), train loss 0.00196
2025-06-23 11:09:17 Training for epoch 70 done, starting evaluation
2025-06-23 11:09:17 Epoch 70/200 (lr=0.0005), train loss 0.00161, valid loss 0.00602
2025-06-23 11:09:17 Model performance:
2025-06-23 11:09:17   metrics/test.rmse:          29.78
2025-06-23 11:09:17   metrics/test.rmse_pcutoff:   3.27
2025-06-23 11:09:17   metrics/test.mAP:           60.52
2025-06-23 11:09:17   metrics/test.mAR:           62.00
2025-06-23 11:09:21 Epoch 71/200 (lr=0.0005), train loss 0.00162
2025-06-23 11:09:25 Epoch 72/200 (lr=0.0005), train loss 0.00128
2025-06-23 11:09:28 Epoch 73/200 (lr=0.0005), train loss 0.00126
2025-06-23 11:09:32 Epoch 74/200 (lr=0.0005), train loss 0.00138
2025-06-23 11:09:36 Epoch 75/200 (lr=0.0005), train loss 0.00145
2025-06-23 11:09:40 Epoch 76/200 (lr=0.0005), train loss 0.00122
2025-06-23 11:09:43 Epoch 77/200 (lr=0.0005), train loss 0.00123
2025-06-23 11:09:47 Epoch 78/200 (lr=0.0005), train loss 0.00133
2025-06-23 11:09:51 Epoch 79/200 (lr=0.0005), train loss 0.00124
2025-06-23 11:09:54 Training for epoch 80 done, starting evaluation
2025-06-23 11:09:55 Epoch 80/200 (lr=0.0005), train loss 0.00121, valid loss 0.00438
2025-06-23 11:09:55 Model performance:
2025-06-23 11:09:55   metrics/test.rmse:           3.60
2025-06-23 11:09:55   metrics/test.rmse_pcutoff:   2.32
2025-06-23 11:09:55   metrics/test.mAP:           92.08
2025-06-23 11:09:55   metrics/test.mAR:           92.00
2025-06-23 11:09:58 Epoch 81/200 (lr=0.0005), train loss 0.00133
2025-06-23 11:10:02 Epoch 82/200 (lr=0.0005), train loss 0.00123
2025-06-23 11:10:06 Epoch 83/200 (lr=0.0005), train loss 0.00120
2025-06-23 11:10:10 Epoch 84/200 (lr=0.0005), train loss 0.00119
2025-06-23 11:10:13 Epoch 85/200 (lr=0.0005), train loss 0.00128
2025-06-23 11:10:17 Epoch 86/200 (lr=0.0005), train loss 0.00133
2025-06-23 11:10:21 Epoch 87/200 (lr=0.0005), train loss 0.00094
2025-06-23 11:10:24 Epoch 88/200 (lr=0.0005), train loss 0.00093
2025-06-23 11:10:28 Epoch 89/200 (lr=0.0005), train loss 0.00230
2025-06-23 11:10:32 Training for epoch 90 done, starting evaluation
2025-06-23 11:10:32 Epoch 90/200 (lr=0.0001), train loss 0.00128, valid loss 0.00565
2025-06-23 11:10:32 Model performance:
2025-06-23 11:10:32   metrics/test.rmse:          20.86
2025-06-23 11:10:32   metrics/test.rmse_pcutoff:   3.19
2025-06-23 11:10:32   metrics/test.mAP:           59.66
2025-06-23 11:10:32   metrics/test.mAR:           64.00
2025-06-23 11:10:35 Epoch 91/200 (lr=0.0001), train loss 0.00125
2025-06-23 11:10:39 Epoch 92/200 (lr=0.0001), train loss 0.00107
2025-06-23 11:10:43 Epoch 93/200 (lr=0.0001), train loss 0.00096
2025-06-23 11:10:46 Epoch 94/200 (lr=0.0001), train loss 0.00097
2025-06-23 11:10:50 Epoch 95/200 (lr=0.0001), train loss 0.00089
2025-06-23 11:10:54 Epoch 96/200 (lr=0.0001), train loss 0.00094
2025-06-23 11:10:58 Epoch 97/200 (lr=0.0001), train loss 0.00090
2025-06-23 11:11:01 Epoch 98/200 (lr=0.0001), train loss 0.00205
2025-06-23 11:11:05 Epoch 99/200 (lr=0.0001), train loss 0.00217
2025-06-23 11:11:09 Training for epoch 100 done, starting evaluation
2025-06-23 11:11:09 Epoch 100/200 (lr=0.0001), train loss 0.00139, valid loss 0.00478
2025-06-23 11:11:09 Model performance:
2025-06-23 11:11:09   metrics/test.rmse:          17.90
2025-06-23 11:11:09   metrics/test.rmse_pcutoff:   2.44
2025-06-23 11:11:09   metrics/test.mAP:           59.27
2025-06-23 11:11:09   metrics/test.mAR:           64.00
2025-06-23 11:11:13 Epoch 101/200 (lr=0.0001), train loss 0.00095
2025-06-23 11:11:17 Epoch 102/200 (lr=0.0001), train loss 0.00097
2025-06-23 11:11:20 Epoch 103/200 (lr=0.0001), train loss 0.00108
2025-06-23 11:11:24 Epoch 104/200 (lr=0.0001), train loss 0.00090
2025-06-23 11:11:28 Epoch 105/200 (lr=0.0001), train loss 0.00113
2025-06-23 11:11:32 Epoch 106/200 (lr=0.0001), train loss 0.00087
2025-06-23 11:11:35 Epoch 107/200 (lr=0.0001), train loss 0.00090
2025-06-23 11:11:39 Epoch 108/200 (lr=0.0001), train loss 0.00090
2025-06-23 11:11:43 Epoch 109/200 (lr=0.0001), train loss 0.00105
2025-06-23 11:11:46 Training for epoch 110 done, starting evaluation
2025-06-23 11:11:47 Epoch 110/200 (lr=0.0001), train loss 0.00090, valid loss 0.00480
2025-06-23 11:11:47 Model performance:
2025-06-23 11:11:47   metrics/test.rmse:          17.67
2025-06-23 11:11:47   metrics/test.rmse_pcutoff:   2.21
2025-06-23 11:11:47   metrics/test.mAP:           59.27
2025-06-23 11:11:47   metrics/test.mAR:           64.00
2025-06-23 11:11:50 Epoch 111/200 (lr=0.0001), train loss 0.00093
2025-06-23 11:11:54 Epoch 112/200 (lr=0.0001), train loss 0.00201
2025-06-23 11:11:58 Epoch 113/200 (lr=0.0001), train loss 0.00103
2025-06-23 11:12:01 Epoch 114/200 (lr=0.0001), train loss 0.00079
2025-06-23 11:12:05 Epoch 115/200 (lr=0.0001), train loss 0.00083
2025-06-23 11:12:09 Epoch 116/200 (lr=0.0001), train loss 0.00091
2025-06-23 11:12:13 Epoch 117/200 (lr=0.0001), train loss 0.00072
2025-06-23 11:12:17 Epoch 118/200 (lr=0.0001), train loss 0.00088
2025-06-23 11:12:20 Epoch 119/200 (lr=0.0001), train loss 0.00078
2025-06-23 11:12:24 Training for epoch 120 done, starting evaluation
2025-06-23 11:12:25 Epoch 120/200 (lr=1e-05), train loss 0.00079, valid loss 0.00501
2025-06-23 11:12:25 Model performance:
2025-06-23 11:12:25   metrics/test.rmse:          18.03
2025-06-23 11:12:25   metrics/test.rmse_pcutoff:   2.20
2025-06-23 11:12:25   metrics/test.mAP:           59.27
2025-06-23 11:12:25   metrics/test.mAR:           64.00
2025-06-23 11:12:28 Epoch 121/200 (lr=1e-05), train loss 0.00093
2025-06-23 11:12:32 Epoch 122/200 (lr=1e-05), train loss 0.00081
2025-06-23 11:12:36 Epoch 123/200 (lr=1e-05), train loss 0.00087
2025-06-23 11:12:40 Epoch 124/200 (lr=1e-05), train loss 0.00081
2025-06-23 11:12:44 Epoch 125/200 (lr=1e-05), train loss 0.00103
2025-06-23 11:12:47 Epoch 126/200 (lr=1e-05), train loss 0.00071
2025-06-23 11:12:51 Epoch 127/200 (lr=1e-05), train loss 0.00083
2025-06-23 11:12:55 Epoch 128/200 (lr=1e-05), train loss 0.00075
2025-06-23 11:12:59 Epoch 129/200 (lr=1e-05), train loss 0.00085
2025-06-23 11:13:02 Training for epoch 130 done, starting evaluation
2025-06-23 11:13:03 Epoch 130/200 (lr=1e-05), train loss 0.00079, valid loss 0.00485
2025-06-23 11:13:03 Model performance:
2025-06-23 11:13:03   metrics/test.rmse:          17.95
2025-06-23 11:13:03   metrics/test.rmse_pcutoff:   2.19
2025-06-23 11:13:03   metrics/test.mAP:           59.27
2025-06-23 11:13:03   metrics/test.mAR:           64.00
2025-06-23 11:13:06 Epoch 131/200 (lr=1e-05), train loss 0.00087
2025-06-23 11:13:10 Epoch 132/200 (lr=1e-05), train loss 0.00087
2025-06-23 11:13:14 Epoch 133/200 (lr=1e-05), train loss 0.00083
2025-06-23 11:13:18 Epoch 134/200 (lr=1e-05), train loss 0.00094
2025-06-23 11:13:22 Epoch 135/200 (lr=1e-05), train loss 0.00072
2025-06-23 11:13:26 Epoch 136/200 (lr=1e-05), train loss 0.00090
2025-06-23 11:13:29 Epoch 137/200 (lr=1e-05), train loss 0.00088
2025-06-23 11:13:33 Epoch 138/200 (lr=1e-05), train loss 0.00074
2025-06-23 11:13:37 Epoch 139/200 (lr=1e-05), train loss 0.00075
2025-06-23 11:13:40 Training for epoch 140 done, starting evaluation
2025-06-23 11:13:41 Epoch 140/200 (lr=1e-05), train loss 0.00204, valid loss 0.00479
2025-06-23 11:13:41 Model performance:
2025-06-23 11:13:41   metrics/test.rmse:          17.90
2025-06-23 11:13:41   metrics/test.rmse_pcutoff:   2.16
2025-06-23 11:13:41   metrics/test.mAP:           59.27
2025-06-23 11:13:41   metrics/test.mAR:           64.00
2025-06-23 11:13:44 Epoch 141/200 (lr=1e-05), train loss 0.00082
2025-06-23 11:13:48 Epoch 142/200 (lr=1e-05), train loss 0.00176
2025-06-23 11:13:52 Epoch 143/200 (lr=1e-05), train loss 0.00091
2025-06-23 11:13:55 Epoch 144/200 (lr=1e-05), train loss 0.00071
2025-06-23 11:13:59 Epoch 145/200 (lr=1e-05), train loss 0.00076
2025-06-23 11:14:03 Epoch 146/200 (lr=1e-05), train loss 0.00192
2025-06-23 11:14:07 Epoch 147/200 (lr=1e-05), train loss 0.00089
2025-06-23 11:14:10 Epoch 148/200 (lr=1e-05), train loss 0.00072
2025-06-23 11:14:14 Epoch 149/200 (lr=1e-05), train loss 0.00079
2025-06-23 11:14:17 Training for epoch 150 done, starting evaluation
2025-06-23 11:14:18 Epoch 150/200 (lr=1e-05), train loss 0.00082, valid loss 0.00481
2025-06-23 11:14:18 Model performance:
2025-06-23 11:14:18   metrics/test.rmse:          17.90
2025-06-23 11:14:18   metrics/test.rmse_pcutoff:   2.18
2025-06-23 11:14:18   metrics/test.mAP:           59.27
2025-06-23 11:14:18   metrics/test.mAR:           64.00
2025-06-23 11:14:21 Epoch 151/200 (lr=1e-05), train loss 0.00080
2025-06-23 11:14:25 Epoch 152/200 (lr=1e-05), train loss 0.00082
2025-06-23 11:14:29 Epoch 153/200 (lr=1e-05), train loss 0.00086
2025-06-23 11:14:32 Epoch 154/200 (lr=1e-05), train loss 0.00088
2025-06-23 11:14:36 Epoch 155/200 (lr=1e-05), train loss 0.00070
2025-06-23 11:14:40 Epoch 156/200 (lr=1e-05), train loss 0.00097
2025-06-23 11:14:43 Epoch 157/200 (lr=1e-05), train loss 0.00066
2025-06-23 11:14:47 Epoch 158/200 (lr=1e-05), train loss 0.00078
2025-06-23 11:14:51 Epoch 159/200 (lr=1e-05), train loss 0.00082
2025-06-23 11:14:55 Training for epoch 160 done, starting evaluation
2025-06-23 11:14:55 Epoch 160/200 (lr=1e-05), train loss 0.00068, valid loss 0.00478
2025-06-23 11:14:55 Model performance:
2025-06-23 11:14:55   metrics/test.rmse:          18.01
2025-06-23 11:14:55   metrics/test.rmse_pcutoff:   2.23
2025-06-23 11:14:55   metrics/test.mAP:           59.27
2025-06-23 11:14:55   metrics/test.mAR:           64.00
2025-06-23 11:14:59 Epoch 161/200 (lr=1e-05), train loss 0.00076
2025-06-23 11:15:02 Epoch 162/200 (lr=1e-05), train loss 0.00080
2025-06-23 11:15:06 Epoch 163/200 (lr=1e-05), train loss 0.00094
2025-06-23 11:15:10 Epoch 164/200 (lr=1e-05), train loss 0.00094
2025-06-23 11:15:13 Epoch 165/200 (lr=1e-05), train loss 0.00070
2025-06-23 11:15:17 Epoch 166/200 (lr=1e-05), train loss 0.00085
2025-06-23 11:15:21 Epoch 167/200 (lr=1e-05), train loss 0.00078
2025-06-23 11:15:24 Epoch 168/200 (lr=1e-05), train loss 0.00082
2025-06-23 11:15:28 Epoch 169/200 (lr=1e-05), train loss 0.00079
2025-06-23 11:15:32 Training for epoch 170 done, starting evaluation
2025-06-23 11:15:32 Epoch 170/200 (lr=1e-05), train loss 0.00084, valid loss 0.00482
2025-06-23 11:15:32 Model performance:
2025-06-23 11:15:32   metrics/test.rmse:          17.96
2025-06-23 11:15:32   metrics/test.rmse_pcutoff:   2.27
2025-06-23 11:15:32   metrics/test.mAP:           59.27
2025-06-23 11:15:32   metrics/test.mAR:           64.00
2025-06-23 11:15:35 Epoch 171/200 (lr=1e-05), train loss 0.00103
2025-06-23 11:15:39 Epoch 172/200 (lr=1e-05), train loss 0.00094
2025-06-23 11:15:43 Epoch 173/200 (lr=1e-05), train loss 0.00071
2025-06-23 11:15:47 Epoch 174/200 (lr=1e-05), train loss 0.00080
2025-06-23 11:15:50 Epoch 175/200 (lr=1e-05), train loss 0.00067
2025-06-23 11:15:54 Epoch 176/200 (lr=1e-05), train loss 0.00088
2025-06-23 11:15:58 Epoch 177/200 (lr=1e-05), train loss 0.00180
2025-06-23 11:16:02 Epoch 178/200 (lr=1e-05), train loss 0.00088
2025-06-23 11:16:06 Epoch 179/200 (lr=1e-05), train loss 0.00089
2025-06-23 11:16:10 Training for epoch 180 done, starting evaluation
2025-06-23 11:16:10 Epoch 180/200 (lr=1e-05), train loss 0.00086, valid loss 0.00481
2025-06-23 11:16:10 Model performance:
2025-06-23 11:16:10   metrics/test.rmse:          17.96
2025-06-23 11:16:10   metrics/test.rmse_pcutoff:   2.26
2025-06-23 11:16:10   metrics/test.mAP:           59.27
2025-06-23 11:16:10   metrics/test.mAR:           64.00
2025-06-23 11:16:14 Epoch 181/200 (lr=1e-05), train loss 0.00070
2025-06-23 11:16:17 Epoch 182/200 (lr=1e-05), train loss 0.00074
2025-06-23 11:16:21 Epoch 183/200 (lr=1e-05), train loss 0.00084
2025-06-23 11:16:25 Epoch 184/200 (lr=1e-05), train loss 0.00084
2025-06-23 11:16:28 Epoch 185/200 (lr=1e-05), train loss 0.00088
2025-06-23 11:16:32 Epoch 186/200 (lr=1e-05), train loss 0.00079
2025-06-23 11:16:36 Epoch 187/200 (lr=1e-05), train loss 0.00077
2025-06-23 11:16:40 Epoch 188/200 (lr=1e-05), train loss 0.00088
2025-06-23 11:16:43 Epoch 189/200 (lr=1e-05), train loss 0.00075
2025-06-23 11:16:47 Training for epoch 190 done, starting evaluation
2025-06-23 11:16:47 Epoch 190/200 (lr=1e-05), train loss 0.00072, valid loss 0.00476
2025-06-23 11:16:47 Model performance:
2025-06-23 11:16:47   metrics/test.rmse:          17.96
2025-06-23 11:16:47   metrics/test.rmse_pcutoff:   2.23
2025-06-23 11:16:47   metrics/test.mAP:           59.27
2025-06-23 11:16:47   metrics/test.mAR:           64.00
2025-06-23 11:16:51 Epoch 191/200 (lr=1e-05), train loss 0.00084
2025-06-23 11:16:55 Epoch 192/200 (lr=1e-05), train loss 0.00087
2025-06-23 11:16:59 Epoch 193/200 (lr=1e-05), train loss 0.00105
2025-06-23 11:17:03 Epoch 194/200 (lr=1e-05), train loss 0.00093
2025-06-23 11:17:06 Epoch 195/200 (lr=1e-05), train loss 0.00089
2025-06-23 11:17:10 Epoch 196/200 (lr=1e-05), train loss 0.00077
2025-06-23 11:17:14 Epoch 197/200 (lr=1e-05), train loss 0.00084
2025-06-23 11:17:18 Epoch 198/200 (lr=1e-05), train loss 0.00066
2025-06-23 11:17:22 Epoch 199/200 (lr=1e-05), train loss 0.00085
2025-06-23 11:17:25 Training for epoch 200 done, starting evaluation
2025-06-23 11:17:26 Epoch 200/200 (lr=1e-05), train loss 0.00073, valid loss 0.00473
2025-06-23 11:17:26 Model performance:
2025-06-23 11:17:26   metrics/test.rmse:          17.96
2025-06-23 11:17:26   metrics/test.rmse_pcutoff:   2.25
2025-06-23 11:17:26   metrics/test.mAP:           59.27
2025-06-23 11:17:26   metrics/test.mAR:           64.00
2025-06-23 14:42:45 Training with configuration:
2025-06-23 14:42:45 data:
2025-06-23 14:42:45   bbox_margin: 20
2025-06-23 14:42:45   colormode: RGB
2025-06-23 14:42:45   inference:
2025-06-23 14:42:45     normalize_images: True
2025-06-23 14:42:45   train:
2025-06-23 14:42:45     affine:
2025-06-23 14:42:45       p: 0.5
2025-06-23 14:42:45       rotation: 30
2025-06-23 14:42:45       scaling: [0.5, 1.25]
2025-06-23 14:42:45       translation: 0
2025-06-23 14:42:45     crop_sampling:
2025-06-23 14:42:45       width: 448
2025-06-23 14:42:45       height: 448
2025-06-23 14:42:45       max_shift: 0.1
2025-06-23 14:42:45       method: hybrid
2025-06-23 14:42:45     gaussian_noise: 12.75
2025-06-23 14:42:45     motion_blur: True
2025-06-23 14:42:45     normalize_images: True
2025-06-23 14:42:45 device: auto
2025-06-23 14:42:45 metadata:
2025-06-23 14:42:45   project_path: /Users/rishi/Library/Application Support/Mountain Duck/Volumes.noindex/transfer.ccv.brown.edu – SFTP.localized/zebrafish_analysis/Manual-Selection/singlelarvae_imaging-rishi-2025-06-13
2025-06-23 14:42:45   pose_config_path: /Users/rishi/Library/Application Support/Mountain Duck/Volumes.noindex/transfer.ccv.brown.edu – SFTP.localized/zebrafish_analysis/Manual-Selection/singlelarvae_imaging-rishi-2025-06-13/dlc-models-pytorch/iteration-0/singlelarvae_imagingJun13-trainset95shuffle1/train/pytorch_config.yaml
2025-06-23 14:42:45   bodyparts: ['left_eye', 'right_eye', 'middle', 'tail', 'well_center']
2025-06-23 14:42:45   unique_bodyparts: []
2025-06-23 14:42:45   individuals: ['animal']
2025-06-23 14:42:45   with_identity: None
2025-06-23 14:42:45 method: bu
2025-06-23 14:42:45 model:
2025-06-23 14:42:45   backbone:
2025-06-23 14:42:45     type: ResNet
2025-06-23 14:42:45     model_name: resnet50_gn
2025-06-23 14:42:45     output_stride: 16
2025-06-23 14:42:45     freeze_bn_stats: False
2025-06-23 14:42:45     freeze_bn_weights: False
2025-06-23 14:42:45   backbone_output_channels: 2048
2025-06-23 14:42:45   heads:
2025-06-23 14:42:45     bodypart:
2025-06-23 14:42:45       type: HeatmapHead
2025-06-23 14:42:45       weight_init: normal
2025-06-23 14:42:45       predictor:
2025-06-23 14:42:45         type: HeatmapPredictor
2025-06-23 14:42:45         apply_sigmoid: False
2025-06-23 14:42:45         clip_scores: True
2025-06-23 14:42:45         location_refinement: True
2025-06-23 14:42:45         locref_std: 7.2801
2025-06-23 14:42:45       target_generator:
2025-06-23 14:42:45         type: HeatmapGaussianGenerator
2025-06-23 14:42:45         num_heatmaps: 5
2025-06-23 14:42:45         pos_dist_thresh: 17
2025-06-23 14:42:45         heatmap_mode: KEYPOINT
2025-06-23 14:42:45         gradient_masking: False
2025-06-23 14:42:45         generate_locref: True
2025-06-23 14:42:45         locref_std: 7.2801
2025-06-23 14:42:45       criterion:
2025-06-23 14:42:45         heatmap:
2025-06-23 14:42:45           type: WeightedMSECriterion
2025-06-23 14:42:45           weight: 1.0
2025-06-23 14:42:45         locref:
2025-06-23 14:42:45           type: WeightedHuberCriterion
2025-06-23 14:42:45           weight: 0.05
2025-06-23 14:42:45       heatmap_config:
2025-06-23 14:42:45         channels: [2048, 5]
2025-06-23 14:42:45         kernel_size: [3]
2025-06-23 14:42:45         strides: [2]
2025-06-23 14:42:45       locref_config:
2025-06-23 14:42:45         channels: [2048, 10]
2025-06-23 14:42:45         kernel_size: [3]
2025-06-23 14:42:45         strides: [2]
2025-06-23 14:42:45 net_type: resnet_50
2025-06-23 14:42:45 runner:
2025-06-23 14:42:45   type: PoseTrainingRunner
2025-06-23 14:42:45   gpus: None
2025-06-23 14:42:45   key_metric: test.mAP
2025-06-23 14:42:45   key_metric_asc: True
2025-06-23 14:42:45   eval_interval: 10
2025-06-23 14:42:45   optimizer:
2025-06-23 14:42:45     type: AdamW
2025-06-23 14:42:45     params:
2025-06-23 14:42:45       lr: 0.0005
2025-06-23 14:42:45   scheduler:
2025-06-23 14:42:45     type: LRListScheduler
2025-06-23 14:42:45     params:
2025-06-23 14:42:45       lr_list: [[0.0001], [1e-05]]
2025-06-23 14:42:45       milestones: [90, 120]
2025-06-23 14:42:45   snapshots:
2025-06-23 14:42:45     max_snapshots: 5
2025-06-23 14:42:45     save_epochs: 25
2025-06-23 14:42:45     save_optimizer_state: False
2025-06-23 14:42:45 train_settings:
2025-06-23 14:42:45   batch_size: 8
2025-06-23 14:42:45   dataloader_workers: 0
2025-06-23 14:42:45   dataloader_pin_memory: False
2025-06-23 14:42:45   display_iters: 500
2025-06-23 14:42:45   epochs: 200
2025-06-23 14:42:45   seed: 42
2025-06-23 14:42:46 Loading pretrained weights from Hugging Face hub (timm/resnet50_gn.a1h_in1k)
2025-06-23 14:42:46 [timm/resnet50_gn.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-06-23 14:42:46 Data Transforms:
2025-06-23 14:42:46   Training:   Compose([
  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (0.5, 1.25), 'y': (0.5, 1.25)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),
  PadIfNeeded(always_apply=True, p=1.0, min_height=448, min_width=448, pad_height_divisor=None, pad_width_divisor=None, position=PositionType.CENTER, border_mode=0, value=None, mask_value=None),
  KeypointAwareCrop(always_apply=True, p=1.0, width=448, height=448, max_shift=0.1, crop_sampling='hybrid'),
  MotionBlur(always_apply=False, p=0.5, blur_limit=(3, 7), allow_shifted=True),
  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2025-06-23 14:42:46   Validation: Compose([
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2025-06-23 14:42:46 Using 89 images and 5 for testing
2025-06-23 14:42:46 
Starting pose model training...
--------------------------------------------------
2025-06-23 14:42:50 Epoch 1/200 (lr=0.0005), train loss 0.01491
2025-06-23 14:42:54 Epoch 2/200 (lr=0.0005), train loss 0.01323
2025-06-23 14:42:58 Epoch 3/200 (lr=0.0005), train loss 0.01100
2025-06-23 14:43:02 Epoch 4/200 (lr=0.0005), train loss 0.00876
2025-06-23 14:43:05 Epoch 5/200 (lr=0.0005), train loss 0.00869
2025-06-23 14:43:09 Epoch 6/200 (lr=0.0005), train loss 0.00754
2025-06-23 14:43:13 Epoch 7/200 (lr=0.0005), train loss 0.00645
2025-06-23 14:43:17 Epoch 8/200 (lr=0.0005), train loss 0.00548
2025-06-23 14:43:20 Epoch 9/200 (lr=0.0005), train loss 0.00511
2025-06-23 14:43:24 Training for epoch 10 done, starting evaluation
2025-06-23 14:43:24 Epoch 10/200 (lr=0.0005), train loss 0.00436, valid loss 0.00838
2025-06-23 14:43:24 Model performance:
2025-06-23 14:43:24   metrics/test.rmse:          26.59
2025-06-23 14:43:24   metrics/test.rmse_pcutoff:   5.92
2025-06-23 14:43:24   metrics/test.mAP:           54.36
2025-06-23 14:43:24   metrics/test.mAR:           56.00
2025-06-23 14:43:28 Epoch 11/200 (lr=0.0005), train loss 0.00416
2025-06-23 14:43:32 Epoch 12/200 (lr=0.0005), train loss 0.00427
2025-06-23 14:43:35 Epoch 13/200 (lr=0.0005), train loss 0.00339
2025-06-23 14:43:39 Epoch 14/200 (lr=0.0005), train loss 0.00300
2025-06-23 14:43:43 Epoch 15/200 (lr=0.0005), train loss 0.00257
2025-06-23 14:43:46 Epoch 16/200 (lr=0.0005), train loss 0.00245
2025-06-23 14:43:50 Epoch 17/200 (lr=0.0005), train loss 0.00343
2025-06-23 14:43:54 Epoch 18/200 (lr=0.0005), train loss 0.00267
2025-06-23 14:43:58 Epoch 19/200 (lr=0.0005), train loss 0.00274
2025-06-23 14:44:01 Training for epoch 20 done, starting evaluation
2025-06-23 14:44:02 Epoch 20/200 (lr=0.0005), train loss 0.00253, valid loss 0.00614
2025-06-23 14:44:02 Model performance:
2025-06-23 14:44:02   metrics/test.rmse:          25.59
2025-06-23 14:44:02   metrics/test.rmse_pcutoff:   2.07
2025-06-23 14:44:02   metrics/test.mAP:           70.56
2025-06-23 14:44:02   metrics/test.mAR:           76.00
2025-06-23 14:44:05 Epoch 21/200 (lr=0.0005), train loss 0.00237
2025-06-23 14:44:09 Epoch 22/200 (lr=0.0005), train loss 0.00235
2025-06-23 14:44:13 Epoch 23/200 (lr=0.0005), train loss 0.00190
2025-06-23 14:44:16 Epoch 24/200 (lr=0.0005), train loss 0.00192
2025-06-23 14:44:20 Epoch 25/200 (lr=0.0005), train loss 0.00177
2025-06-23 14:44:24 Epoch 26/200 (lr=0.0005), train loss 0.00190
2025-06-23 14:44:28 Epoch 27/200 (lr=0.0005), train loss 0.00271
2025-06-23 14:44:32 Epoch 28/200 (lr=0.0005), train loss 0.00213
2025-06-23 14:44:35 Epoch 29/200 (lr=0.0005), train loss 0.00204
2025-06-23 14:44:39 Training for epoch 30 done, starting evaluation
2025-06-23 14:44:40 Epoch 30/200 (lr=0.0005), train loss 0.00178, valid loss 0.00541
2025-06-23 14:44:40 Model performance:
2025-06-23 14:44:40   metrics/test.rmse:          14.76
2025-06-23 14:44:40   metrics/test.rmse_pcutoff:   3.47
2025-06-23 14:44:40   metrics/test.mAP:           72.07
2025-06-23 14:44:40   metrics/test.mAR:           78.00
2025-06-23 14:44:43 Epoch 31/200 (lr=0.0005), train loss 0.00196
2025-06-23 14:44:47 Epoch 32/200 (lr=0.0005), train loss 0.00187
2025-06-23 14:44:51 Epoch 33/200 (lr=0.0005), train loss 0.00168
2025-06-23 14:44:54 Epoch 34/200 (lr=0.0005), train loss 0.00213
2025-06-23 14:44:58 Epoch 35/200 (lr=0.0005), train loss 0.00175
2025-06-23 14:45:02 Epoch 36/200 (lr=0.0005), train loss 0.00185
2025-06-23 14:45:06 Epoch 37/200 (lr=0.0005), train loss 0.00171
2025-06-23 14:45:09 Epoch 38/200 (lr=0.0005), train loss 0.00171
2025-06-23 14:45:13 Epoch 39/200 (lr=0.0005), train loss 0.00168
2025-06-23 14:45:17 Training for epoch 40 done, starting evaluation
2025-06-23 14:45:17 Epoch 40/200 (lr=0.0005), train loss 0.00154, valid loss 0.00499
2025-06-23 14:45:17 Model performance:
2025-06-23 14:45:17   metrics/test.rmse:          15.46
2025-06-23 14:45:17   metrics/test.rmse_pcutoff:   2.63
2025-06-23 14:45:17   metrics/test.mAP:           69.63
2025-06-23 14:45:17   metrics/test.mAR:           76.00
2025-06-23 14:45:20 Epoch 41/200 (lr=0.0005), train loss 0.00177
2025-06-23 14:45:24 Epoch 42/200 (lr=0.0005), train loss 0.00187
2025-06-23 14:45:28 Epoch 43/200 (lr=0.0005), train loss 0.00169
2025-06-23 14:45:32 Epoch 44/200 (lr=0.0005), train loss 0.00177
2025-06-23 14:45:35 Epoch 45/200 (lr=0.0005), train loss 0.00161
2025-06-23 14:45:39 Epoch 46/200 (lr=0.0005), train loss 0.00146
2025-06-23 14:45:43 Epoch 47/200 (lr=0.0005), train loss 0.00146
2025-06-23 14:45:46 Epoch 48/200 (lr=0.0005), train loss 0.00154
2025-06-23 14:45:50 Epoch 49/200 (lr=0.0005), train loss 0.00151
2025-06-23 14:45:54 Training for epoch 50 done, starting evaluation
2025-06-23 14:45:54 Epoch 50/200 (lr=0.0005), train loss 0.00176, valid loss 0.00433
2025-06-23 14:45:54 Model performance:
2025-06-23 14:45:54   metrics/test.rmse:          14.00
2025-06-23 14:45:54   metrics/test.rmse_pcutoff:   2.69
2025-06-23 14:45:54   metrics/test.mAP:           72.70
2025-06-23 14:45:54   metrics/test.mAR:           82.00
2025-06-23 14:45:58 Epoch 51/200 (lr=0.0005), train loss 0.00138
2025-06-23 14:46:01 Epoch 52/200 (lr=0.0005), train loss 0.00140
2025-06-23 14:46:05 Epoch 53/200 (lr=0.0005), train loss 0.00138
2025-06-23 14:46:09 Epoch 54/200 (lr=0.0005), train loss 0.00138
2025-06-23 14:46:13 Epoch 55/200 (lr=0.0005), train loss 0.00140
2025-06-23 14:46:16 Epoch 56/200 (lr=0.0005), train loss 0.00139
2025-06-23 14:46:20 Epoch 57/200 (lr=0.0005), train loss 0.00131
2025-06-23 14:46:24 Epoch 58/200 (lr=0.0005), train loss 0.00124
2025-06-23 14:46:28 Epoch 59/200 (lr=0.0005), train loss 0.00128
2025-06-23 14:46:31 Training for epoch 60 done, starting evaluation
2025-06-23 14:46:32 Epoch 60/200 (lr=0.0005), train loss 0.00137, valid loss 0.00421
2025-06-23 14:46:32 Model performance:
2025-06-23 14:46:32   metrics/test.rmse:           7.38
2025-06-23 14:46:32   metrics/test.rmse_pcutoff:   2.63
2025-06-23 14:46:32   metrics/test.mAP:           79.41
2025-06-23 14:46:32   metrics/test.mAR:           80.00
2025-06-23 14:46:35 Epoch 61/200 (lr=0.0005), train loss 0.00115
2025-06-23 14:46:39 Epoch 62/200 (lr=0.0005), train loss 0.00116
2025-06-23 14:46:42 Epoch 63/200 (lr=0.0005), train loss 0.00148
2025-06-23 14:46:46 Epoch 64/200 (lr=0.0005), train loss 0.00140
2025-06-23 14:46:50 Epoch 65/200 (lr=0.0005), train loss 0.00139
2025-06-23 14:46:53 Epoch 66/200 (lr=0.0005), train loss 0.00143
2025-06-23 14:46:57 Epoch 67/200 (lr=0.0005), train loss 0.00151
2025-06-23 14:47:01 Epoch 68/200 (lr=0.0005), train loss 0.00176
2025-06-23 14:47:05 Epoch 69/200 (lr=0.0005), train loss 0.00196
2025-06-23 14:47:08 Training for epoch 70 done, starting evaluation
2025-06-23 14:47:09 Epoch 70/200 (lr=0.0005), train loss 0.00161, valid loss 0.00602
2025-06-23 14:47:09 Model performance:
2025-06-23 14:47:09   metrics/test.rmse:          29.78
2025-06-23 14:47:09   metrics/test.rmse_pcutoff:   3.27
2025-06-23 14:47:09   metrics/test.mAP:           60.52
2025-06-23 14:47:09   metrics/test.mAR:           62.00
2025-06-23 14:47:12 Epoch 71/200 (lr=0.0005), train loss 0.00162
2025-06-23 14:47:16 Epoch 72/200 (lr=0.0005), train loss 0.00128
2025-06-23 14:47:20 Epoch 73/200 (lr=0.0005), train loss 0.00126
2025-06-23 14:47:23 Epoch 74/200 (lr=0.0005), train loss 0.00138
2025-06-23 14:47:27 Epoch 75/200 (lr=0.0005), train loss 0.00145
2025-06-23 14:47:31 Epoch 76/200 (lr=0.0005), train loss 0.00122
2025-06-23 14:47:35 Epoch 77/200 (lr=0.0005), train loss 0.00123
2025-06-23 14:47:38 Epoch 78/200 (lr=0.0005), train loss 0.00133
2025-06-23 14:47:42 Epoch 79/200 (lr=0.0005), train loss 0.00124
2025-06-23 14:47:46 Training for epoch 80 done, starting evaluation
2025-06-23 14:47:46 Epoch 80/200 (lr=0.0005), train loss 0.00121, valid loss 0.00438
2025-06-23 14:47:46 Model performance:
2025-06-23 14:47:46   metrics/test.rmse:           3.60
2025-06-23 14:47:46   metrics/test.rmse_pcutoff:   2.32
2025-06-23 14:47:46   metrics/test.mAP:           92.08
2025-06-23 14:47:46   metrics/test.mAR:           92.00
2025-06-23 14:47:50 Epoch 81/200 (lr=0.0005), train loss 0.00133
2025-06-23 14:47:54 Epoch 82/200 (lr=0.0005), train loss 0.00123
2025-06-23 14:47:57 Epoch 83/200 (lr=0.0005), train loss 0.00120
2025-06-23 14:48:01 Epoch 84/200 (lr=0.0005), train loss 0.00119
2025-06-23 14:48:05 Epoch 85/200 (lr=0.0005), train loss 0.00128
2025-06-23 14:48:09 Epoch 86/200 (lr=0.0005), train loss 0.00133
2025-06-23 14:48:12 Epoch 87/200 (lr=0.0005), train loss 0.00094
2025-06-23 14:48:16 Epoch 88/200 (lr=0.0005), train loss 0.00093
2025-06-23 14:48:20 Epoch 89/200 (lr=0.0005), train loss 0.00230
2025-06-23 14:48:23 Training for epoch 90 done, starting evaluation
2025-06-23 14:48:23 Epoch 90/200 (lr=0.0001), train loss 0.00128, valid loss 0.00565
2025-06-23 14:48:23 Model performance:
2025-06-23 14:48:23   metrics/test.rmse:          20.86
2025-06-23 14:48:23   metrics/test.rmse_pcutoff:   3.19
2025-06-23 14:48:23   metrics/test.mAP:           59.66
2025-06-23 14:48:23   metrics/test.mAR:           64.00
2025-06-23 14:48:27 Epoch 91/200 (lr=0.0001), train loss 0.00125
2025-06-23 14:48:31 Epoch 92/200 (lr=0.0001), train loss 0.00107
2025-06-23 14:48:35 Epoch 93/200 (lr=0.0001), train loss 0.00096
2025-06-23 14:48:38 Epoch 94/200 (lr=0.0001), train loss 0.00097
2025-06-23 14:48:42 Epoch 95/200 (lr=0.0001), train loss 0.00089
2025-06-23 14:48:45 Epoch 96/200 (lr=0.0001), train loss 0.00094
2025-06-23 14:48:49 Epoch 97/200 (lr=0.0001), train loss 0.00090
2025-06-23 14:48:53 Epoch 98/200 (lr=0.0001), train loss 0.00205
2025-06-23 14:48:57 Epoch 99/200 (lr=0.0001), train loss 0.00217
2025-06-23 14:49:00 Training for epoch 100 done, starting evaluation
2025-06-23 14:49:01 Epoch 100/200 (lr=0.0001), train loss 0.00139, valid loss 0.00478
2025-06-23 14:49:01 Model performance:
2025-06-23 14:49:01   metrics/test.rmse:          17.90
2025-06-23 14:49:01   metrics/test.rmse_pcutoff:   2.44
2025-06-23 14:49:01   metrics/test.mAP:           59.27
2025-06-23 14:49:01   metrics/test.mAR:           64.00
2025-06-23 14:49:05 Epoch 101/200 (lr=0.0001), train loss 0.00095
2025-06-23 14:49:08 Epoch 102/200 (lr=0.0001), train loss 0.00097
2025-06-23 14:49:12 Epoch 103/200 (lr=0.0001), train loss 0.00108
2025-06-23 14:49:16 Epoch 104/200 (lr=0.0001), train loss 0.00090
2025-06-23 14:49:19 Epoch 105/200 (lr=0.0001), train loss 0.00113
2025-06-23 14:49:23 Epoch 106/200 (lr=0.0001), train loss 0.00087
2025-06-23 14:49:27 Epoch 107/200 (lr=0.0001), train loss 0.00090
2025-06-23 14:49:30 Epoch 108/200 (lr=0.0001), train loss 0.00090
2025-06-23 14:49:34 Epoch 109/200 (lr=0.0001), train loss 0.00105
2025-06-23 14:49:38 Training for epoch 110 done, starting evaluation
2025-06-23 14:49:38 Epoch 110/200 (lr=0.0001), train loss 0.00090, valid loss 0.00480
2025-06-23 14:49:38 Model performance:
2025-06-23 14:49:38   metrics/test.rmse:          17.67
2025-06-23 14:49:38   metrics/test.rmse_pcutoff:   2.21
2025-06-23 14:49:38   metrics/test.mAP:           59.27
2025-06-23 14:49:38   metrics/test.mAR:           64.00
2025-06-23 14:49:42 Epoch 111/200 (lr=0.0001), train loss 0.00093
2025-06-23 14:49:45 Epoch 112/200 (lr=0.0001), train loss 0.00201
2025-06-23 14:49:49 Epoch 113/200 (lr=0.0001), train loss 0.00103
2025-06-23 14:49:53 Epoch 114/200 (lr=0.0001), train loss 0.00079
2025-06-23 14:49:57 Epoch 115/200 (lr=0.0001), train loss 0.00083
2025-06-23 14:50:00 Epoch 116/200 (lr=0.0001), train loss 0.00091
2025-06-23 14:50:04 Epoch 117/200 (lr=0.0001), train loss 0.00072
2025-06-23 14:50:08 Epoch 118/200 (lr=0.0001), train loss 0.00088
2025-06-23 14:50:11 Epoch 119/200 (lr=0.0001), train loss 0.00078
2025-06-23 14:50:15 Training for epoch 120 done, starting evaluation
2025-06-23 14:50:15 Epoch 120/200 (lr=1e-05), train loss 0.00079, valid loss 0.00501
2025-06-23 14:50:15 Model performance:
2025-06-23 14:50:15   metrics/test.rmse:          18.03
2025-06-23 14:50:15   metrics/test.rmse_pcutoff:   2.20
2025-06-23 14:50:15   metrics/test.mAP:           59.27
2025-06-23 14:50:15   metrics/test.mAR:           64.00
2025-06-23 14:50:19 Epoch 121/200 (lr=1e-05), train loss 0.00093
2025-06-23 14:50:23 Epoch 122/200 (lr=1e-05), train loss 0.00081
2025-06-23 14:50:27 Epoch 123/200 (lr=1e-05), train loss 0.00087
2025-06-23 14:50:30 Epoch 124/200 (lr=1e-05), train loss 0.00081
2025-06-23 14:50:34 Epoch 125/200 (lr=1e-05), train loss 0.00103
2025-06-23 14:50:38 Epoch 126/200 (lr=1e-05), train loss 0.00071
2025-06-23 14:50:42 Epoch 127/200 (lr=1e-05), train loss 0.00083
2025-06-23 14:50:45 Epoch 128/200 (lr=1e-05), train loss 0.00075
2025-06-23 14:50:49 Epoch 129/200 (lr=1e-05), train loss 0.00085
2025-06-23 14:50:53 Training for epoch 130 done, starting evaluation
2025-06-23 14:50:53 Epoch 130/200 (lr=1e-05), train loss 0.00079, valid loss 0.00485
2025-06-23 14:50:53 Model performance:
2025-06-23 14:50:53   metrics/test.rmse:          17.95
2025-06-23 14:50:53   metrics/test.rmse_pcutoff:   2.19
2025-06-23 14:50:53   metrics/test.mAP:           59.27
2025-06-23 14:50:53   metrics/test.mAR:           64.00
2025-06-23 14:50:57 Epoch 131/200 (lr=1e-05), train loss 0.00087
2025-06-23 14:51:00 Epoch 132/200 (lr=1e-05), train loss 0.00087
2025-06-23 14:51:04 Epoch 133/200 (lr=1e-05), train loss 0.00083
2025-06-23 14:51:08 Epoch 134/200 (lr=1e-05), train loss 0.00094
2025-06-23 14:51:12 Epoch 135/200 (lr=1e-05), train loss 0.00072
2025-06-23 14:51:16 Epoch 136/200 (lr=1e-05), train loss 0.00090
2025-06-23 14:51:19 Epoch 137/200 (lr=1e-05), train loss 0.00088
2025-06-23 14:51:23 Epoch 138/200 (lr=1e-05), train loss 0.00074
2025-06-23 14:51:27 Epoch 139/200 (lr=1e-05), train loss 0.00075
2025-06-23 14:51:30 Training for epoch 140 done, starting evaluation
2025-06-23 14:51:31 Epoch 140/200 (lr=1e-05), train loss 0.00204, valid loss 0.00479
2025-06-23 14:51:31 Model performance:
2025-06-23 14:51:31   metrics/test.rmse:          17.90
2025-06-23 14:51:31   metrics/test.rmse_pcutoff:   2.16
2025-06-23 14:51:31   metrics/test.mAP:           59.27
2025-06-23 14:51:31   metrics/test.mAR:           64.00
2025-06-23 14:51:34 Epoch 141/200 (lr=1e-05), train loss 0.00082
2025-06-23 14:51:38 Epoch 142/200 (lr=1e-05), train loss 0.00176
2025-06-23 14:51:42 Epoch 143/200 (lr=1e-05), train loss 0.00091
2025-06-23 14:51:45 Epoch 144/200 (lr=1e-05), train loss 0.00071
2025-06-23 14:51:49 Epoch 145/200 (lr=1e-05), train loss 0.00076
2025-06-23 14:51:53 Epoch 146/200 (lr=1e-05), train loss 0.00192
2025-06-23 14:51:57 Epoch 147/200 (lr=1e-05), train loss 0.00089
2025-06-23 14:52:00 Epoch 148/200 (lr=1e-05), train loss 0.00072
2025-06-23 14:52:04 Epoch 149/200 (lr=1e-05), train loss 0.00079
2025-06-23 14:52:08 Training for epoch 150 done, starting evaluation
2025-06-23 14:52:08 Epoch 150/200 (lr=1e-05), train loss 0.00082, valid loss 0.00481
2025-06-23 14:52:08 Model performance:
2025-06-23 14:52:08   metrics/test.rmse:          17.90
2025-06-23 14:52:08   metrics/test.rmse_pcutoff:   2.18
2025-06-23 14:52:08   metrics/test.mAP:           59.27
2025-06-23 14:52:08   metrics/test.mAR:           64.00
2025-06-23 14:52:12 Epoch 151/200 (lr=1e-05), train loss 0.00080
2025-06-23 14:52:15 Epoch 152/200 (lr=1e-05), train loss 0.00082
2025-06-23 14:52:19 Epoch 153/200 (lr=1e-05), train loss 0.00086
2025-06-23 14:52:23 Epoch 154/200 (lr=1e-05), train loss 0.00088
2025-06-23 14:52:27 Epoch 155/200 (lr=1e-05), train loss 0.00070
2025-06-23 14:52:30 Epoch 156/200 (lr=1e-05), train loss 0.00097
2025-06-23 14:52:34 Epoch 157/200 (lr=1e-05), train loss 0.00066
2025-06-23 14:52:38 Epoch 158/200 (lr=1e-05), train loss 0.00078
2025-06-23 14:52:42 Epoch 159/200 (lr=1e-05), train loss 0.00082
2025-06-23 14:52:46 Training for epoch 160 done, starting evaluation
2025-06-23 14:52:46 Epoch 160/200 (lr=1e-05), train loss 0.00068, valid loss 0.00478
2025-06-23 14:52:46 Model performance:
2025-06-23 14:52:46   metrics/test.rmse:          18.01
2025-06-23 14:52:46   metrics/test.rmse_pcutoff:   2.23
2025-06-23 14:52:46   metrics/test.mAP:           59.27
2025-06-23 14:52:46   metrics/test.mAR:           64.00
2025-06-23 14:52:50 Epoch 161/200 (lr=1e-05), train loss 0.00076
2025-06-23 14:52:53 Epoch 162/200 (lr=1e-05), train loss 0.00080
2025-06-23 14:52:57 Epoch 163/200 (lr=1e-05), train loss 0.00094
2025-06-23 14:53:01 Epoch 164/200 (lr=1e-05), train loss 0.00094
2025-06-23 14:53:04 Epoch 165/200 (lr=1e-05), train loss 0.00070
2025-06-23 14:53:08 Epoch 166/200 (lr=1e-05), train loss 0.00085
2025-06-23 14:53:12 Epoch 167/200 (lr=1e-05), train loss 0.00078
2025-06-23 14:53:15 Epoch 168/200 (lr=1e-05), train loss 0.00082
2025-06-23 14:53:19 Epoch 169/200 (lr=1e-05), train loss 0.00079
2025-06-23 14:53:23 Training for epoch 170 done, starting evaluation
2025-06-23 14:53:23 Epoch 170/200 (lr=1e-05), train loss 0.00084, valid loss 0.00482
2025-06-23 14:53:23 Model performance:
2025-06-23 14:53:23   metrics/test.rmse:          17.96
2025-06-23 14:53:23   metrics/test.rmse_pcutoff:   2.27
2025-06-23 14:53:23   metrics/test.mAP:           59.27
2025-06-23 14:53:23   metrics/test.mAR:           64.00
2025-06-23 14:53:26 Epoch 171/200 (lr=1e-05), train loss 0.00103
2025-06-23 14:53:30 Epoch 172/200 (lr=1e-05), train loss 0.00094
2025-06-23 14:53:34 Epoch 173/200 (lr=1e-05), train loss 0.00071
2025-06-23 14:53:38 Epoch 174/200 (lr=1e-05), train loss 0.00080
2025-06-23 14:53:41 Epoch 175/200 (lr=1e-05), train loss 0.00067
2025-06-23 14:53:45 Epoch 176/200 (lr=1e-05), train loss 0.00088
2025-06-23 14:53:49 Epoch 177/200 (lr=1e-05), train loss 0.00180
2025-06-23 14:53:53 Epoch 178/200 (lr=1e-05), train loss 0.00088
2025-06-23 14:53:57 Epoch 179/200 (lr=1e-05), train loss 0.00089
2025-06-23 14:54:00 Training for epoch 180 done, starting evaluation
2025-06-23 14:54:01 Epoch 180/200 (lr=1e-05), train loss 0.00086, valid loss 0.00481
2025-06-23 14:54:01 Model performance:
2025-06-23 14:54:01   metrics/test.rmse:          17.96
2025-06-23 14:54:01   metrics/test.rmse_pcutoff:   2.26
2025-06-23 14:54:01   metrics/test.mAP:           59.27
2025-06-23 14:54:01   metrics/test.mAR:           64.00
2025-06-23 14:54:04 Epoch 181/200 (lr=1e-05), train loss 0.00070
2025-06-23 14:54:08 Epoch 182/200 (lr=1e-05), train loss 0.00074
2025-06-23 14:54:12 Epoch 183/200 (lr=1e-05), train loss 0.00084
2025-06-23 14:54:16 Epoch 184/200 (lr=1e-05), train loss 0.00084
2025-06-23 14:54:19 Epoch 185/200 (lr=1e-05), train loss 0.00088
2025-06-23 14:54:23 Epoch 186/200 (lr=1e-05), train loss 0.00079
2025-06-23 14:54:27 Epoch 187/200 (lr=1e-05), train loss 0.00077
2025-06-23 14:54:30 Epoch 188/200 (lr=1e-05), train loss 0.00088
2025-06-23 14:54:34 Epoch 189/200 (lr=1e-05), train loss 0.00075
2025-06-23 14:54:38 Training for epoch 190 done, starting evaluation
2025-06-23 14:54:38 Epoch 190/200 (lr=1e-05), train loss 0.00072, valid loss 0.00476
2025-06-23 14:54:38 Model performance:
2025-06-23 14:54:38   metrics/test.rmse:          17.96
2025-06-23 14:54:38   metrics/test.rmse_pcutoff:   2.23
2025-06-23 14:54:38   metrics/test.mAP:           59.27
2025-06-23 14:54:38   metrics/test.mAR:           64.00
2025-06-23 14:54:42 Epoch 191/200 (lr=1e-05), train loss 0.00084
2025-06-23 14:54:45 Epoch 192/200 (lr=1e-05), train loss 0.00087
2025-06-23 14:54:49 Epoch 193/200 (lr=1e-05), train loss 0.00105
2025-06-23 14:54:53 Epoch 194/200 (lr=1e-05), train loss 0.00093
2025-06-23 14:54:56 Epoch 195/200 (lr=1e-05), train loss 0.00089
2025-06-23 14:55:00 Epoch 196/200 (lr=1e-05), train loss 0.00077
2025-06-23 14:55:04 Epoch 197/200 (lr=1e-05), train loss 0.00084
2025-06-23 14:55:08 Epoch 198/200 (lr=1e-05), train loss 0.00066
2025-06-23 14:55:12 Epoch 199/200 (lr=1e-05), train loss 0.00085
2025-06-23 14:55:15 Training for epoch 200 done, starting evaluation
2025-06-23 14:55:16 Epoch 200/200 (lr=1e-05), train loss 0.00073, valid loss 0.00473
2025-06-23 14:55:16 Model performance:
2025-06-23 14:55:16   metrics/test.rmse:          17.96
2025-06-23 14:55:16   metrics/test.rmse_pcutoff:   2.25
2025-06-23 14:55:16   metrics/test.mAP:           59.27
2025-06-23 14:55:16   metrics/test.mAR:           64.00
2025-06-25 07:58:47 Training with configuration:
2025-06-25 07:58:47 data:
2025-06-25 07:58:47   bbox_margin: 20
2025-06-25 07:58:47   colormode: RGB
2025-06-25 07:58:47   inference:
2025-06-25 07:58:47     normalize_images: True
2025-06-25 07:58:47   train:
2025-06-25 07:58:47     affine:
2025-06-25 07:58:47       p: 0.5
2025-06-25 07:58:47       rotation: 30
2025-06-25 07:58:47       scaling: [0.5, 1.25]
2025-06-25 07:58:47       translation: 0
2025-06-25 07:58:47     crop_sampling:
2025-06-25 07:58:47       width: 448
2025-06-25 07:58:47       height: 448
2025-06-25 07:58:47       max_shift: 0.1
2025-06-25 07:58:47       method: hybrid
2025-06-25 07:58:47     gaussian_noise: 12.75
2025-06-25 07:58:47     motion_blur: True
2025-06-25 07:58:47     normalize_images: True
2025-06-25 07:58:47 device: auto
2025-06-25 07:58:47 metadata:
2025-06-25 07:58:47   project_path: /Users/rishi/Library/Application Support/Mountain Duck/Volumes.noindex/transfer.ccv.brown.edu – SFTP.localized/zebrafish_analysis/Manual-Selection/singlelarvae_imaging-rishi-2025-06-13
2025-06-25 07:58:47   pose_config_path: /Users/rishi/Library/Application Support/Mountain Duck/Volumes.noindex/transfer.ccv.brown.edu – SFTP.localized/zebrafish_analysis/Manual-Selection/singlelarvae_imaging-rishi-2025-06-13/dlc-models-pytorch/iteration-0/singlelarvae_imagingJun13-trainset95shuffle1/train/pytorch_config.yaml
2025-06-25 07:58:47   bodyparts: ['left_eye', 'right_eye', 'middle', 'tail', 'well_center']
2025-06-25 07:58:47   unique_bodyparts: []
2025-06-25 07:58:47   individuals: ['animal']
2025-06-25 07:58:47   with_identity: None
2025-06-25 07:58:47 method: bu
2025-06-25 07:58:47 model:
2025-06-25 07:58:47   backbone:
2025-06-25 07:58:47     type: ResNet
2025-06-25 07:58:47     model_name: resnet50_gn
2025-06-25 07:58:47     output_stride: 16
2025-06-25 07:58:47     freeze_bn_stats: False
2025-06-25 07:58:47     freeze_bn_weights: False
2025-06-25 07:58:47   backbone_output_channels: 2048
2025-06-25 07:58:47   heads:
2025-06-25 07:58:47     bodypart:
2025-06-25 07:58:47       type: HeatmapHead
2025-06-25 07:58:47       weight_init: normal
2025-06-25 07:58:47       predictor:
2025-06-25 07:58:47         type: HeatmapPredictor
2025-06-25 07:58:47         apply_sigmoid: False
2025-06-25 07:58:47         clip_scores: True
2025-06-25 07:58:47         location_refinement: True
2025-06-25 07:58:47         locref_std: 7.2801
2025-06-25 07:58:47       target_generator:
2025-06-25 07:58:47         type: HeatmapGaussianGenerator
2025-06-25 07:58:47         num_heatmaps: 5
2025-06-25 07:58:47         pos_dist_thresh: 17
2025-06-25 07:58:47         heatmap_mode: KEYPOINT
2025-06-25 07:58:47         gradient_masking: False
2025-06-25 07:58:47         generate_locref: True
2025-06-25 07:58:47         locref_std: 7.2801
2025-06-25 07:58:47       criterion:
2025-06-25 07:58:47         heatmap:
2025-06-25 07:58:47           type: WeightedMSECriterion
2025-06-25 07:58:47           weight: 1.0
2025-06-25 07:58:47         locref:
2025-06-25 07:58:47           type: WeightedHuberCriterion
2025-06-25 07:58:47           weight: 0.05
2025-06-25 07:58:47       heatmap_config:
2025-06-25 07:58:47         channels: [2048, 5]
2025-06-25 07:58:47         kernel_size: [3]
2025-06-25 07:58:47         strides: [2]
2025-06-25 07:58:47       locref_config:
2025-06-25 07:58:47         channels: [2048, 10]
2025-06-25 07:58:47         kernel_size: [3]
2025-06-25 07:58:47         strides: [2]
2025-06-25 07:58:47 net_type: resnet_50
2025-06-25 07:58:47 runner:
2025-06-25 07:58:47   type: PoseTrainingRunner
2025-06-25 07:58:47   gpus: None
2025-06-25 07:58:47   key_metric: test.mAP
2025-06-25 07:58:47   key_metric_asc: True
2025-06-25 07:58:47   eval_interval: 10
2025-06-25 07:58:47   optimizer:
2025-06-25 07:58:47     type: AdamW
2025-06-25 07:58:47     params:
2025-06-25 07:58:47       lr: 0.0005
2025-06-25 07:58:47   scheduler:
2025-06-25 07:58:47     type: LRListScheduler
2025-06-25 07:58:47     params:
2025-06-25 07:58:47       lr_list: [[0.0001], [1e-05]]
2025-06-25 07:58:47       milestones: [90, 120]
2025-06-25 07:58:47   snapshots:
2025-06-25 07:58:47     max_snapshots: 5
2025-06-25 07:58:47     save_epochs: 25
2025-06-25 07:58:47     save_optimizer_state: False
2025-06-25 07:58:47 train_settings:
2025-06-25 07:58:47   batch_size: 8
2025-06-25 07:58:47   dataloader_workers: 0
2025-06-25 07:58:47   dataloader_pin_memory: False
2025-06-25 07:58:47   display_iters: 500
2025-06-25 07:58:47   epochs: 200
2025-06-25 07:58:47   seed: 42
2025-06-25 07:58:48 Loading pretrained weights from Hugging Face hub (timm/resnet50_gn.a1h_in1k)
2025-06-25 07:58:48 [timm/resnet50_gn.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-06-25 07:58:49 Data Transforms:
2025-06-25 07:58:49   Training:   Compose([
  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (0.5, 1.25), 'y': (0.5, 1.25)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),
  PadIfNeeded(always_apply=True, p=1.0, min_height=448, min_width=448, pad_height_divisor=None, pad_width_divisor=None, position=PositionType.CENTER, border_mode=0, value=None, mask_value=None),
  KeypointAwareCrop(always_apply=True, p=1.0, width=448, height=448, max_shift=0.1, crop_sampling='hybrid'),
  MotionBlur(always_apply=False, p=0.5, blur_limit=(3, 7), allow_shifted=True),
  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2025-06-25 07:58:49   Validation: Compose([
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
2025-06-25 07:58:49 Using 108 images and 6 for testing
2025-06-25 07:58:49 
Starting pose model training...
--------------------------------------------------
2025-06-25 07:58:56 Epoch 1/200 (lr=0.0005), train loss 0.01510
2025-06-25 07:59:01 Epoch 2/200 (lr=0.0005), train loss 0.01236
2025-06-25 07:59:05 Epoch 3/200 (lr=0.0005), train loss 0.01026
2025-06-25 07:59:10 Epoch 4/200 (lr=0.0005), train loss 0.00935
2025-06-25 07:59:15 Epoch 5/200 (lr=0.0005), train loss 0.00849
2025-06-25 07:59:19 Epoch 6/200 (lr=0.0005), train loss 0.00696
2025-06-25 07:59:24 Epoch 7/200 (lr=0.0005), train loss 0.00622
2025-06-25 07:59:29 Epoch 8/200 (lr=0.0005), train loss 0.00523
2025-06-25 07:59:33 Epoch 9/200 (lr=0.0005), train loss 0.00457
2025-06-25 07:59:38 Training for epoch 10 done, starting evaluation
2025-06-25 07:59:39 Epoch 10/200 (lr=0.0005), train loss 0.00412, valid loss 0.00668
2025-06-25 07:59:39 Model performance:
2025-06-25 07:59:39   metrics/test.rmse:         127.76
2025-06-25 07:59:39   metrics/test.rmse_pcutoff: 327.45
2025-06-25 07:59:39   metrics/test.mAP:           49.16
2025-06-25 07:59:39   metrics/test.mAR:           61.67
2025-06-25 07:59:43 Epoch 11/200 (lr=0.0005), train loss 0.00369
2025-06-25 07:59:48 Epoch 12/200 (lr=0.0005), train loss 0.00326
2025-06-25 07:59:53 Epoch 13/200 (lr=0.0005), train loss 0.00306
2025-06-25 07:59:57 Epoch 14/200 (lr=0.0005), train loss 0.00303
2025-06-25 08:00:02 Epoch 15/200 (lr=0.0005), train loss 0.00285
2025-06-25 08:00:07 Epoch 16/200 (lr=0.0005), train loss 0.00314
2025-06-25 08:00:11 Epoch 17/200 (lr=0.0005), train loss 0.00274
2025-06-25 08:00:16 Epoch 18/200 (lr=0.0005), train loss 0.00231
2025-06-25 08:00:21 Epoch 19/200 (lr=0.0005), train loss 0.00256
2025-06-25 08:00:25 Training for epoch 20 done, starting evaluation
2025-06-25 08:00:26 Epoch 20/200 (lr=0.0005), train loss 0.00252, valid loss 0.00424
2025-06-25 08:00:26 Model performance:
2025-06-25 08:00:26   metrics/test.rmse:         103.73
2025-06-25 08:00:26   metrics/test.rmse_pcutoff: 112.62
2025-06-25 08:00:26   metrics/test.mAP:           61.02
2025-06-25 08:00:26   metrics/test.mAR:           73.33
2025-06-25 08:00:30 Epoch 21/200 (lr=0.0005), train loss 0.00233
2025-06-25 08:00:35 Epoch 22/200 (lr=0.0005), train loss 0.00244
2025-06-25 08:00:40 Epoch 23/200 (lr=0.0005), train loss 0.00232
2025-06-25 08:00:45 Epoch 24/200 (lr=0.0005), train loss 0.00234
2025-06-25 08:00:50 Epoch 25/200 (lr=0.0005), train loss 0.00223
2025-06-25 08:00:54 Epoch 26/200 (lr=0.0005), train loss 0.00224
2025-06-25 08:00:59 Epoch 27/200 (lr=0.0005), train loss 0.00245
2025-06-25 08:01:04 Epoch 28/200 (lr=0.0005), train loss 0.00226
2025-06-25 08:01:08 Epoch 29/200 (lr=0.0005), train loss 0.00225
2025-06-25 08:01:13 Training for epoch 30 done, starting evaluation
2025-06-25 08:01:13 Epoch 30/200 (lr=0.0005), train loss 0.00227, valid loss 0.00425
2025-06-25 08:01:13 Model performance:
2025-06-25 08:01:13   metrics/test.rmse:          98.31
2025-06-25 08:01:13   metrics/test.rmse_pcutoff:   2.56
2025-06-25 08:01:13   metrics/test.mAP:           59.23
2025-06-25 08:01:13   metrics/test.mAR:           70.00
2025-06-25 08:01:18 Epoch 31/200 (lr=0.0005), train loss 0.00216
2025-06-25 08:01:23 Epoch 32/200 (lr=0.0005), train loss 0.00209
2025-06-25 08:01:27 Epoch 33/200 (lr=0.0005), train loss 0.00190
2025-06-25 08:01:32 Epoch 34/200 (lr=0.0005), train loss 0.00199
2025-06-25 08:01:37 Epoch 35/200 (lr=0.0005), train loss 0.00195
2025-06-25 08:01:42 Epoch 36/200 (lr=0.0005), train loss 0.00204
2025-06-25 08:01:46 Epoch 37/200 (lr=0.0005), train loss 0.00201
2025-06-25 08:01:51 Epoch 38/200 (lr=0.0005), train loss 0.00219
2025-06-25 08:01:56 Epoch 39/200 (lr=0.0005), train loss 0.00204
2025-06-25 08:02:00 Training for epoch 40 done, starting evaluation
2025-06-25 08:02:00 Epoch 40/200 (lr=0.0005), train loss 0.00210, valid loss 0.00469
2025-06-25 08:02:00 Model performance:
2025-06-25 08:02:00   metrics/test.rmse:          53.66
2025-06-25 08:02:00   metrics/test.rmse_pcutoff:   2.97
2025-06-25 08:02:00   metrics/test.mAP:           59.09
2025-06-25 08:02:00   metrics/test.mAR:           68.33
2025-06-25 08:02:05 Epoch 41/200 (lr=0.0005), train loss 0.00189
2025-06-25 08:02:10 Epoch 42/200 (lr=0.0005), train loss 0.00172
2025-06-25 08:02:14 Epoch 43/200 (lr=0.0005), train loss 0.00188
2025-06-25 08:02:19 Epoch 44/200 (lr=0.0005), train loss 0.00185
2025-06-25 08:02:24 Epoch 45/200 (lr=0.0005), train loss 0.00163
2025-06-25 08:02:29 Epoch 46/200 (lr=0.0005), train loss 0.00194
2025-06-25 08:02:33 Epoch 47/200 (lr=0.0005), train loss 0.00185
2025-06-25 08:02:38 Epoch 48/200 (lr=0.0005), train loss 0.00182
2025-06-25 08:02:43 Epoch 49/200 (lr=0.0005), train loss 0.00178
2025-06-25 08:02:47 Training for epoch 50 done, starting evaluation
2025-06-25 08:02:48 Epoch 50/200 (lr=0.0005), train loss 0.00178, valid loss 0.00362
2025-06-25 08:02:48 Model performance:
2025-06-25 08:02:48   metrics/test.rmse:          52.05
2025-06-25 08:02:48   metrics/test.rmse_pcutoff:  59.75
2025-06-25 08:02:48   metrics/test.mAP:           61.62
2025-06-25 08:02:48   metrics/test.mAR:           73.33
2025-06-25 08:02:52 Epoch 51/200 (lr=0.0005), train loss 0.00176
2025-06-25 08:02:57 Epoch 52/200 (lr=0.0005), train loss 0.00165
2025-06-25 08:03:01 Epoch 53/200 (lr=0.0005), train loss 0.00191
2025-06-25 08:03:06 Epoch 54/200 (lr=0.0005), train loss 0.00198
2025-06-25 08:03:11 Epoch 55/200 (lr=0.0005), train loss 0.00197
2025-06-25 08:03:15 Epoch 56/200 (lr=0.0005), train loss 0.00191
2025-06-25 08:03:20 Epoch 57/200 (lr=0.0005), train loss 0.00173
2025-06-25 08:03:25 Epoch 58/200 (lr=0.0005), train loss 0.00167
2025-06-25 08:03:29 Epoch 59/200 (lr=0.0005), train loss 0.00166
2025-06-25 08:03:34 Training for epoch 60 done, starting evaluation
2025-06-25 08:03:34 Epoch 60/200 (lr=0.0005), train loss 0.00173, valid loss 0.00452
2025-06-25 08:03:34 Model performance:
2025-06-25 08:03:34   metrics/test.rmse:          48.04
2025-06-25 08:03:34   metrics/test.rmse_pcutoff:  57.78
2025-06-25 08:03:34   metrics/test.mAP:           59.11
2025-06-25 08:03:34   metrics/test.mAR:           71.67
2025-06-25 08:03:39 Epoch 61/200 (lr=0.0005), train loss 0.00188
2025-06-25 08:03:44 Epoch 62/200 (lr=0.0005), train loss 0.00182
2025-06-25 08:03:49 Epoch 63/200 (lr=0.0005), train loss 0.00183
2025-06-25 08:03:53 Epoch 64/200 (lr=0.0005), train loss 0.00172
2025-06-25 08:03:58 Epoch 65/200 (lr=0.0005), train loss 0.00167
2025-06-25 08:04:03 Epoch 66/200 (lr=0.0005), train loss 0.00164
2025-06-25 08:04:07 Epoch 67/200 (lr=0.0005), train loss 0.00190
2025-06-25 08:04:12 Epoch 68/200 (lr=0.0005), train loss 0.00172
2025-06-25 08:04:17 Epoch 69/200 (lr=0.0005), train loss 0.00180
2025-06-25 08:04:22 Training for epoch 70 done, starting evaluation
2025-06-25 08:04:22 Epoch 70/200 (lr=0.0005), train loss 0.00174, valid loss 0.00353
2025-06-25 08:04:22 Model performance:
2025-06-25 08:04:22   metrics/test.rmse:          69.45
2025-06-25 08:04:22   metrics/test.rmse_pcutoff:  48.42
2025-06-25 08:04:22   metrics/test.mAP:           52.50
2025-06-25 08:04:22   metrics/test.mAR:           68.33
2025-06-25 08:04:27 Epoch 71/200 (lr=0.0005), train loss 0.00166
2025-06-25 08:04:31 Epoch 72/200 (lr=0.0005), train loss 0.00162
2025-06-25 08:04:36 Epoch 73/200 (lr=0.0005), train loss 0.00162
2025-06-25 08:04:41 Epoch 74/200 (lr=0.0005), train loss 0.00147
2025-06-25 08:04:45 Epoch 75/200 (lr=0.0005), train loss 0.00139
2025-06-25 08:04:50 Epoch 76/200 (lr=0.0005), train loss 0.00149
2025-06-25 08:04:55 Epoch 77/200 (lr=0.0005), train loss 0.00156
2025-06-25 08:04:59 Epoch 78/200 (lr=0.0005), train loss 0.00159
2025-06-25 08:05:04 Epoch 79/200 (lr=0.0005), train loss 0.00152
2025-06-25 08:05:09 Training for epoch 80 done, starting evaluation
2025-06-25 08:05:09 Epoch 80/200 (lr=0.0005), train loss 0.00155, valid loss 0.00336
2025-06-25 08:05:09 Model performance:
2025-06-25 08:05:09   metrics/test.rmse:          85.26
2025-06-25 08:05:09   metrics/test.rmse_pcutoff: 104.73
2025-06-25 08:05:09   metrics/test.mAP:           67.50
2025-06-25 08:05:09   metrics/test.mAR:           73.33
2025-06-25 08:05:14 Epoch 81/200 (lr=0.0005), train loss 0.00163
2025-06-25 08:05:19 Epoch 82/200 (lr=0.0005), train loss 0.00168
2025-06-25 08:05:23 Epoch 83/200 (lr=0.0005), train loss 0.00161
2025-06-25 08:05:28 Epoch 84/200 (lr=0.0005), train loss 0.00162
2025-06-25 08:05:32 Epoch 85/200 (lr=0.0005), train loss 0.00173
2025-06-25 08:05:37 Epoch 86/200 (lr=0.0005), train loss 0.00140
2025-06-25 08:05:42 Epoch 87/200 (lr=0.0005), train loss 0.00167
2025-06-25 08:05:46 Epoch 88/200 (lr=0.0005), train loss 0.00165
2025-06-25 08:05:51 Epoch 89/200 (lr=0.0005), train loss 0.00173
2025-06-25 08:05:56 Training for epoch 90 done, starting evaluation
2025-06-25 08:05:56 Epoch 90/200 (lr=0.0001), train loss 0.00166, valid loss 0.00300
2025-06-25 08:05:56 Model performance:
2025-06-25 08:05:56   metrics/test.rmse:          47.57
2025-06-25 08:05:56   metrics/test.rmse_pcutoff:   2.34
2025-06-25 08:05:56   metrics/test.mAP:           69.91
2025-06-25 08:05:56   metrics/test.mAR:           75.00
2025-06-25 08:06:01 Epoch 91/200 (lr=0.0001), train loss 0.00142
2025-06-25 08:06:06 Epoch 92/200 (lr=0.0001), train loss 0.00139
2025-06-25 08:06:10 Epoch 93/200 (lr=0.0001), train loss 0.00159
2025-06-25 08:06:15 Epoch 94/200 (lr=0.0001), train loss 0.00143
2025-06-25 08:06:20 Epoch 95/200 (lr=0.0001), train loss 0.00140
2025-06-25 08:06:24 Epoch 96/200 (lr=0.0001), train loss 0.00112
2025-06-25 08:06:29 Epoch 97/200 (lr=0.0001), train loss 0.00128
2025-06-25 08:06:34 Epoch 98/200 (lr=0.0001), train loss 0.00126
2025-06-25 08:06:39 Epoch 99/200 (lr=0.0001), train loss 0.00137
2025-06-25 08:06:43 Training for epoch 100 done, starting evaluation
2025-06-25 08:06:44 Epoch 100/200 (lr=0.0001), train loss 0.00123, valid loss 0.00291
2025-06-25 08:06:44 Model performance:
2025-06-25 08:06:44   metrics/test.rmse:          84.74
2025-06-25 08:06:44   metrics/test.rmse_pcutoff:  95.37
2025-06-25 08:06:44   metrics/test.mAP:           55.76
2025-06-25 08:06:44   metrics/test.mAR:           71.67
2025-06-25 08:06:48 Epoch 101/200 (lr=0.0001), train loss 0.00134
2025-06-25 08:06:53 Epoch 102/200 (lr=0.0001), train loss 0.00110
2025-06-25 08:06:58 Epoch 103/200 (lr=0.0001), train loss 0.00122
2025-06-25 08:07:03 Epoch 104/200 (lr=0.0001), train loss 0.00114
2025-06-25 08:07:07 Epoch 105/200 (lr=0.0001), train loss 0.00110
2025-06-25 08:07:12 Epoch 106/200 (lr=0.0001), train loss 0.00131
2025-06-25 08:07:17 Epoch 107/200 (lr=0.0001), train loss 0.00120
2025-06-25 08:07:21 Epoch 108/200 (lr=0.0001), train loss 0.00113
2025-06-25 08:07:26 Epoch 109/200 (lr=0.0001), train loss 0.00127
2025-06-25 08:07:31 Training for epoch 110 done, starting evaluation
2025-06-25 08:07:31 Epoch 110/200 (lr=0.0001), train loss 0.00133, valid loss 0.00257
2025-06-25 08:07:31 Model performance:
2025-06-25 08:07:31   metrics/test.rmse:          47.39
2025-06-25 08:07:31   metrics/test.rmse_pcutoff:  51.12
2025-06-25 08:07:31   metrics/test.mAP:           61.34
2025-06-25 08:07:31   metrics/test.mAR:           75.00
2025-06-25 08:07:36 Epoch 111/200 (lr=0.0001), train loss 0.00135
2025-06-25 08:07:41 Epoch 112/200 (lr=0.0001), train loss 0.00128
2025-06-25 08:07:46 Epoch 113/200 (lr=0.0001), train loss 0.00121
2025-06-25 08:07:50 Epoch 114/200 (lr=0.0001), train loss 0.00123
2025-06-25 08:07:55 Epoch 115/200 (lr=0.0001), train loss 0.00120
2025-06-25 08:08:00 Epoch 116/200 (lr=0.0001), train loss 0.00108
2025-06-25 08:08:04 Epoch 117/200 (lr=0.0001), train loss 0.00119
2025-06-25 08:08:09 Epoch 118/200 (lr=0.0001), train loss 0.00119
2025-06-25 08:08:14 Epoch 119/200 (lr=0.0001), train loss 0.00132
2025-06-25 08:08:19 Training for epoch 120 done, starting evaluation
2025-06-25 08:08:19 Epoch 120/200 (lr=1e-05), train loss 0.00121, valid loss 0.00248
2025-06-25 08:08:19 Model performance:
2025-06-25 08:08:19   metrics/test.rmse:          47.16
2025-06-25 08:08:19   metrics/test.rmse_pcutoff:  48.69
2025-06-25 08:08:19   metrics/test.mAP:           64.64
2025-06-25 08:08:19   metrics/test.mAR:           78.33
2025-06-25 08:08:24 Epoch 121/200 (lr=1e-05), train loss 0.00110
2025-06-25 08:08:28 Epoch 122/200 (lr=1e-05), train loss 0.00113
2025-06-25 08:08:33 Epoch 123/200 (lr=1e-05), train loss 0.00116
2025-06-25 08:08:37 Epoch 124/200 (lr=1e-05), train loss 0.00102
2025-06-25 08:08:42 Epoch 125/200 (lr=1e-05), train loss 0.00104
2025-06-25 08:08:47 Epoch 126/200 (lr=1e-05), train loss 0.00106
2025-06-25 08:08:52 Epoch 127/200 (lr=1e-05), train loss 0.00134
2025-06-25 08:08:56 Epoch 128/200 (lr=1e-05), train loss 0.00104
2025-06-25 08:09:01 Epoch 129/200 (lr=1e-05), train loss 0.00137
2025-06-25 08:09:06 Training for epoch 130 done, starting evaluation
2025-06-25 08:09:06 Epoch 130/200 (lr=1e-05), train loss 0.00121, valid loss 0.00254
2025-06-25 08:09:06 Model performance:
2025-06-25 08:09:06   metrics/test.rmse:          57.34
2025-06-25 08:09:06   metrics/test.rmse_pcutoff:  48.64
2025-06-25 08:09:06   metrics/test.mAP:           62.74
2025-06-25 08:09:06   metrics/test.mAR:           76.67
2025-06-25 08:09:11 Epoch 131/200 (lr=1e-05), train loss 0.00113
2025-06-25 08:09:15 Epoch 132/200 (lr=1e-05), train loss 0.00111
2025-06-25 08:09:20 Epoch 133/200 (lr=1e-05), train loss 0.00109
2025-06-25 08:09:25 Epoch 134/200 (lr=1e-05), train loss 0.00108
2025-06-25 08:09:30 Epoch 135/200 (lr=1e-05), train loss 0.00122
2025-06-25 08:09:34 Epoch 136/200 (lr=1e-05), train loss 0.00109
2025-06-25 08:09:39 Epoch 137/200 (lr=1e-05), train loss 0.00109
2025-06-25 08:09:43 Epoch 138/200 (lr=1e-05), train loss 0.00116
2025-06-25 08:09:48 Epoch 139/200 (lr=1e-05), train loss 0.00105
2025-06-25 08:09:53 Training for epoch 140 done, starting evaluation
2025-06-25 08:09:53 Epoch 140/200 (lr=1e-05), train loss 0.00107, valid loss 0.00251
2025-06-25 08:09:53 Model performance:
2025-06-25 08:09:53   metrics/test.rmse:          57.51
2025-06-25 08:09:53   metrics/test.rmse_pcutoff:  48.71
2025-06-25 08:09:53   metrics/test.mAP:           63.23
2025-06-25 08:09:53   metrics/test.mAR:           76.67
2025-06-25 08:09:58 Epoch 141/200 (lr=1e-05), train loss 0.00101
2025-06-25 08:10:02 Epoch 142/200 (lr=1e-05), train loss 0.00115
2025-06-25 08:10:07 Epoch 143/200 (lr=1e-05), train loss 0.00103
2025-06-25 08:10:12 Epoch 144/200 (lr=1e-05), train loss 0.00107
2025-06-25 08:10:16 Epoch 145/200 (lr=1e-05), train loss 0.00104
2025-06-25 08:10:21 Epoch 146/200 (lr=1e-05), train loss 0.00115
2025-06-25 08:10:26 Epoch 147/200 (lr=1e-05), train loss 0.00093
2025-06-25 08:10:31 Epoch 148/200 (lr=1e-05), train loss 0.00112
2025-06-25 08:10:35 Epoch 149/200 (lr=1e-05), train loss 0.00096
2025-06-25 08:10:40 Training for epoch 150 done, starting evaluation
2025-06-25 08:10:41 Epoch 150/200 (lr=1e-05), train loss 0.00106, valid loss 0.00248
2025-06-25 08:10:41 Model performance:
2025-06-25 08:10:41   metrics/test.rmse:          47.16
2025-06-25 08:10:41   metrics/test.rmse_pcutoff:  48.66
2025-06-25 08:10:41   metrics/test.mAP:           63.23
2025-06-25 08:10:41   metrics/test.mAR:           76.67
2025-06-25 08:10:45 Epoch 151/200 (lr=1e-05), train loss 0.00104
2025-06-25 08:10:50 Epoch 152/200 (lr=1e-05), train loss 0.00102
2025-06-25 08:10:55 Epoch 153/200 (lr=1e-05), train loss 0.00102
2025-06-25 08:10:59 Epoch 154/200 (lr=1e-05), train loss 0.00111
2025-06-25 08:11:04 Epoch 155/200 (lr=1e-05), train loss 0.00106
2025-06-25 08:11:09 Epoch 156/200 (lr=1e-05), train loss 0.00099
2025-06-25 08:11:14 Epoch 157/200 (lr=1e-05), train loss 0.00104
2025-06-25 08:11:18 Epoch 158/200 (lr=1e-05), train loss 0.00091
2025-06-25 08:11:23 Epoch 159/200 (lr=1e-05), train loss 0.00105
2025-06-25 08:11:28 Training for epoch 160 done, starting evaluation
2025-06-25 08:11:28 Epoch 160/200 (lr=1e-05), train loss 0.00094, valid loss 0.00244
2025-06-25 08:11:28 Model performance:
2025-06-25 08:11:28   metrics/test.rmse:          24.90
2025-06-25 08:11:28   metrics/test.rmse_pcutoff:   2.11
2025-06-25 08:11:28   metrics/test.mAP:           74.39
2025-06-25 08:11:28   metrics/test.mAR:           83.33
2025-06-25 08:11:33 Epoch 161/200 (lr=1e-05), train loss 0.00119
2025-06-25 08:11:38 Epoch 162/200 (lr=1e-05), train loss 0.00109
2025-06-25 08:11:43 Epoch 163/200 (lr=1e-05), train loss 0.00094
2025-06-25 08:11:47 Epoch 164/200 (lr=1e-05), train loss 0.00111
2025-06-25 08:11:52 Epoch 165/200 (lr=1e-05), train loss 0.00116
2025-06-25 08:11:57 Epoch 166/200 (lr=1e-05), train loss 0.00104
2025-06-25 08:12:02 Epoch 167/200 (lr=1e-05), train loss 0.00091
2025-06-25 08:12:06 Epoch 168/200 (lr=1e-05), train loss 0.00099
2025-06-25 08:12:11 Epoch 169/200 (lr=1e-05), train loss 0.00118
2025-06-25 08:12:16 Training for epoch 170 done, starting evaluation
2025-06-25 08:12:16 Epoch 170/200 (lr=1e-05), train loss 0.00099, valid loss 0.00251
2025-06-25 08:12:16 Model performance:
2025-06-25 08:12:16   metrics/test.rmse:          24.94
2025-06-25 08:12:16   metrics/test.rmse_pcutoff:   2.13
2025-06-25 08:12:16   metrics/test.mAP:           74.39
2025-06-25 08:12:16   metrics/test.mAR:           83.33
2025-06-25 08:12:21 Epoch 171/200 (lr=1e-05), train loss 0.00124
2025-06-25 08:12:26 Epoch 172/200 (lr=1e-05), train loss 0.00093
2025-06-25 08:12:30 Epoch 173/200 (lr=1e-05), train loss 0.00090
2025-06-25 08:12:35 Epoch 174/200 (lr=1e-05), train loss 0.00097
2025-06-25 08:12:40 Epoch 175/200 (lr=1e-05), train loss 0.00106
2025-06-25 08:12:44 Epoch 176/200 (lr=1e-05), train loss 0.00103
2025-06-25 08:12:49 Epoch 177/200 (lr=1e-05), train loss 0.00112
2025-06-25 08:12:54 Epoch 178/200 (lr=1e-05), train loss 0.00110
2025-06-25 08:12:58 Epoch 179/200 (lr=1e-05), train loss 0.00105
2025-06-25 08:13:03 Training for epoch 180 done, starting evaluation
2025-06-25 08:13:03 Epoch 180/200 (lr=1e-05), train loss 0.00106, valid loss 0.00246
2025-06-25 08:13:03 Model performance:
2025-06-25 08:13:03   metrics/test.rmse:          24.89
2025-06-25 08:13:03   metrics/test.rmse_pcutoff:   2.11
2025-06-25 08:13:03   metrics/test.mAP:           74.39
2025-06-25 08:13:03   metrics/test.mAR:           83.33
2025-06-25 08:13:08 Epoch 181/200 (lr=1e-05), train loss 0.00098
2025-06-25 08:13:12 Epoch 182/200 (lr=1e-05), train loss 0.00101
2025-06-25 08:13:17 Epoch 183/200 (lr=1e-05), train loss 0.00113
2025-06-25 08:13:22 Epoch 184/200 (lr=1e-05), train loss 0.00103
2025-06-25 08:13:27 Epoch 185/200 (lr=1e-05), train loss 0.00119
2025-06-25 08:13:31 Epoch 186/200 (lr=1e-05), train loss 0.00102
2025-06-25 08:13:36 Epoch 187/200 (lr=1e-05), train loss 0.00119
2025-06-25 08:13:41 Epoch 188/200 (lr=1e-05), train loss 0.00104
2025-06-25 08:13:46 Epoch 189/200 (lr=1e-05), train loss 0.00108
2025-06-25 08:13:50 Training for epoch 190 done, starting evaluation
2025-06-25 08:13:50 Epoch 190/200 (lr=1e-05), train loss 0.00120, valid loss 0.00249
2025-06-25 08:13:50 Model performance:
2025-06-25 08:13:50   metrics/test.rmse:          24.91
2025-06-25 08:13:50   metrics/test.rmse_pcutoff:   2.08
2025-06-25 08:13:50   metrics/test.mAP:           74.39
2025-06-25 08:13:50   metrics/test.mAR:           83.33
2025-06-25 08:13:55 Epoch 191/200 (lr=1e-05), train loss 0.00110
2025-06-25 08:14:00 Epoch 192/200 (lr=1e-05), train loss 0.00106
2025-06-25 08:14:05 Epoch 193/200 (lr=1e-05), train loss 0.00099
2025-06-25 08:14:09 Epoch 194/200 (lr=1e-05), train loss 0.00106
2025-06-25 08:14:14 Epoch 195/200 (lr=1e-05), train loss 0.00103
2025-06-25 08:14:19 Epoch 196/200 (lr=1e-05), train loss 0.00102
2025-06-25 08:14:23 Epoch 197/200 (lr=1e-05), train loss 0.00101
2025-06-25 08:14:28 Epoch 198/200 (lr=1e-05), train loss 0.00107
2025-06-25 08:14:32 Epoch 199/200 (lr=1e-05), train loss 0.00093
2025-06-25 08:14:37 Training for epoch 200 done, starting evaluation
2025-06-25 08:14:37 Epoch 200/200 (lr=1e-05), train loss 0.00116, valid loss 0.00253
2025-06-25 08:14:37 Model performance:
2025-06-25 08:14:37   metrics/test.rmse:          24.92
2025-06-25 08:14:37   metrics/test.rmse_pcutoff:   2.14
2025-06-25 08:14:37   metrics/test.mAP:           74.39
2025-06-25 08:14:37   metrics/test.mAR:           83.33
